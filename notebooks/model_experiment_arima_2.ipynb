{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a07QSbaIsjVR"
   },
   "source": [
    "#Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21617,
     "status": "ok",
     "timestamp": 1751905259235,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "J6g9bxvIr3QY",
    "outputId": "65d51af1-60f7-457c-cd5b-25ee424e46c0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7537,
     "status": "ok",
     "timestamp": 1751905271101,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "7sh7aztksLl1",
    "outputId": "a753fe95-aeb5-4ff6-b414-a5990d2a9230"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open('/content/drive/MyDrive/MLFinal/git_token.env', 'r') as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "username = \"badrilosaberidze\"\n",
    "\n",
    "%cd /content/drive/MyDrive/MLFinal/walmart-sales-forecasting\n",
    "!git remote set-url origin https://{username}:{token}@github.com/{username}/Walmart-Recruiting---Store-Sales-Forecasting.git\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pyyvky0rsmzw"
   },
   "source": [
    "#Needed Imports & dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9429,
     "status": "ok",
     "timestamp": 1751907110765,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "39XVY2itsv4e",
    "outputId": "5b0926ad-e139-4b19-a02a-564c79ce8d94"
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn statsmodels mlflow dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1751905528103,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "JXBj4wdosUc1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 829,
     "status": "ok",
     "timestamp": 1751905327536,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "hJOBUXjZsaTC"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1031,
     "status": "ok",
     "timestamp": 1751905559986,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "TpN8-ghrsfvP"
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import dagshub\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249,
     "referenced_widgets": [
      "85c3227127314ebd87f8b79517c80839",
      "7b65864b732d4c3dae33e2c126cb6737"
     ]
    },
    "executionInfo": {
     "elapsed": 12871,
     "status": "ok",
     "timestamp": 1751905574022,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "YT-lDxV9tC9u",
    "outputId": "6a8def44-a54e-41f5-feb8-5837316a05ca"
   },
   "outputs": [],
   "source": [
    "dagshub.init(repo_owner=\"losaberidzebadri\", repo_name=\"Walmart-Recruiting---Store-Sales-Forecasting\", mlflow=True)\n",
    "mlflow.set_experiment(\"walmart-sales-forecasting\")\n",
    "\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PejBcWypv70r"
   },
   "source": [
    "#Data Loading And Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1751906196365,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "7Ic2BU81txRl"
   },
   "outputs": [],
   "source": [
    "def load_and_explore_data(train_path, test_path, features_path, stores_path):\n",
    "    \"\"\"\n",
    "    Load all datasets and perform initial exploration\n",
    "    \"\"\"\n",
    "    print(\"Loading datasets...\")\n",
    "\n",
    "    # Load datasets\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    features_df = pd.read_csv(features_path)\n",
    "    stores_df = pd.read_csv(stores_path)\n",
    "\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "    print(f\"Features data shape: {features_df.shape}\")\n",
    "    print(f\"Stores data shape: {stores_df.shape}\")\n",
    "\n",
    "    # Convert Date column to datetime\n",
    "    train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
    "    test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
    "    features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
    "\n",
    "    # Log basic dataset info\n",
    "    with mlflow.start_run(run_name=\"data_exploration\"):\n",
    "        mlflow.log_param(\"train_rows\", train_df.shape[0])\n",
    "        mlflow.log_param(\"train_cols\", train_df.shape[1])\n",
    "        mlflow.log_param(\"test_rows\", test_df.shape[0])\n",
    "        mlflow.log_param(\"departments\", train_df['Dept'].nunique())\n",
    "        mlflow.log_param(\"stores\", train_df['Store'].nunique())\n",
    "        mlflow.log_param(\"date_range\", f\"{train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
    "\n",
    "    return train_df, test_df, features_df, stores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1751906334046,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "1ZfSGUMqv6vG"
   },
   "outputs": [],
   "source": [
    "def explore_department_patterns(train_df):\n",
    "    \"\"\"\n",
    "    Analyze patterns at department level\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Department-Level Analysis ===\")\n",
    "\n",
    "    # Aggregate by department and date\n",
    "    dept_sales = train_df.groupby(['Dept', 'Date'])['Weekly_Sales'].agg(['mean', 'sum', 'count']).reset_index()\n",
    "    dept_sales.columns = ['Dept', 'Date', 'Mean_Sales', 'Total_Sales', 'Store_Count']\n",
    "\n",
    "    # Top departments by total sales\n",
    "    top_depts = dept_sales.groupby('Dept')['Total_Sales'].sum().sort_values(ascending=False).head(10)\n",
    "    print(\"Top 10 departments by total sales:\")\n",
    "    print(top_depts)\n",
    "\n",
    "    # Visualize top departments\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Plot 1: Top departments total sales\n",
    "    plt.subplot(2, 2, 1)\n",
    "    top_depts.plot(kind='bar')\n",
    "    plt.title('Top 10 Departments by Total Sales')\n",
    "    plt.xlabel('Department')\n",
    "    plt.ylabel('Total Sales')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Plot 2: Time series for top 3 departments\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for dept in top_depts.head(3).index:\n",
    "        dept_ts = dept_sales[dept_sales['Dept'] == dept]\n",
    "        plt.plot(dept_ts['Date'], dept_ts['Total_Sales'], label=f'Dept {dept}')\n",
    "    plt.title('Time Series - Top 3 Departments')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Total Sales')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Plot 3: Seasonality check\n",
    "    plt.subplot(2, 2, 3)\n",
    "    # Get weekly patterns\n",
    "    dept_sales['Week'] = dept_sales['Date'].dt.isocalendar().week\n",
    "    weekly_pattern = dept_sales.groupby('Week')['Total_Sales'].mean()\n",
    "    weekly_pattern.plot()\n",
    "    plt.title('Average Weekly Sales Pattern')\n",
    "    plt.xlabel('Week of Year')\n",
    "    plt.ylabel('Average Total Sales')\n",
    "\n",
    "    # Plot 4: Holiday effects\n",
    "    plt.subplot(2, 2, 4)\n",
    "    dept_sales['Month'] = dept_sales['Date'].dt.month\n",
    "    monthly_pattern = dept_sales.groupby('Month')['Total_Sales'].mean()\n",
    "    monthly_pattern.plot(kind='bar')\n",
    "    plt.title('Average Monthly Sales Pattern')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Total Sales')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return dept_sales, top_depts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76-0ZsWqwT1E"
   },
   "source": [
    "#Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1751907735989,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "d02Tp92VwcER"
   },
   "outputs": [],
   "source": [
    "def create_features(train_df, features_df, stores_df):\n",
    "    \"\"\"\n",
    "    Create features for ARIMA modeling\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Feature Engineering ===\")\n",
    "\n",
    "    # Merge with external features\n",
    "    train_enhanced = train_df.merge(features_df, on=['Store', 'Date'], how='left')\n",
    "    train_enhanced = train_enhanced.merge(stores_df, on='Store', how='left')\n",
    "\n",
    "    # Create time-based features\n",
    "    train_enhanced['Year'] = train_enhanced['Date'].dt.year\n",
    "    train_enhanced['Month'] = train_enhanced['Date'].dt.month\n",
    "    train_enhanced['Week'] = train_enhanced['Date'].dt.isocalendar().week\n",
    "    train_enhanced['Quarter'] = train_enhanced['Date'].dt.quarter\n",
    "    train_enhanced['DayOfYear'] = train_enhanced['Date'].dt.dayofyear\n",
    "\n",
    "    # Create lag features (for departments)\n",
    "    dept_features = []\n",
    "\n",
    "    for dept in train_enhanced['Dept'].unique():\n",
    "        dept_data = train_enhanced[train_enhanced['Dept'] == dept].copy()\n",
    "\n",
    "        # Aggregate by date for this department\n",
    "        dept_agg = dept_data.groupby('Date').agg({\n",
    "            'Weekly_Sales': ['mean', 'sum', 'std', 'count'],\n",
    "            'Temperature': 'mean',\n",
    "            'Fuel_Price': 'mean',\n",
    "            'CPI': 'mean',\n",
    "            'Unemployment': 'mean',\n",
    "            'IsHoliday_x': 'max'  # If any store has holiday, mark as holiday\n",
    "        }).reset_index()\n",
    "\n",
    "        # Flatten column names\n",
    "        dept_agg.columns = ['Date', 'Sales_Mean', 'Sales_Sum', 'Sales_Std', 'Store_Count',\n",
    "                           'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'IsHoliday']\n",
    "        dept_agg['Dept'] = dept\n",
    "\n",
    "        # Create lag features\n",
    "        dept_agg = dept_agg.sort_values('Date')\n",
    "        dept_agg['Sales_Lag1'] = dept_agg['Sales_Mean'].shift(1)\n",
    "        dept_agg['Sales_Lag2'] = dept_agg['Sales_Mean'].shift(2)\n",
    "        dept_agg['Sales_Lag4'] = dept_agg['Sales_Mean'].shift(4)\n",
    "        dept_agg['Sales_MA3'] = dept_agg['Sales_Mean'].rolling(window=3).mean()\n",
    "        dept_agg['Sales_MA5'] = dept_agg['Sales_Mean'].rolling(window=5).mean()\n",
    "\n",
    "        # Handle missing values\n",
    "        dept_agg = dept_agg.fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "        dept_features.append(dept_agg)\n",
    "\n",
    "    # Combine all departments\n",
    "    dept_features_df = pd.concat(dept_features, ignore_index=True)\n",
    "\n",
    "    print(f\"Feature engineering complete. Shape: {dept_features_df.shape}\")\n",
    "    print(f\"Features: {dept_features_df.columns.tolist()}\")\n",
    "\n",
    "    return dept_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-LHO445w8YE"
   },
   "source": [
    "#Arima Modeling Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1751906523500,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "IqFUPzkGw-Wz"
   },
   "outputs": [],
   "source": [
    "def check_stationarity(timeseries, title):\n",
    "    \"\"\"\n",
    "    Check if time series is stationary\n",
    "    \"\"\"\n",
    "    print(f'\\n=== Stationarity Test: {title} ===')\n",
    "\n",
    "    # Perform Augmented Dickey-Fuller test\n",
    "    result = adfuller(timeseries.dropna())\n",
    "    print(f'ADF Statistic: {result[0]:.6f}')\n",
    "    print(f'p-value: {result[1]:.6f}')\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value:.3f}')\n",
    "\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Series is stationary\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Series is non-stationary\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1751906538244,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "ZKUxbBILxCdf"
   },
   "outputs": [],
   "source": [
    "def plot_decomposition(timeseries, title, period=52):\n",
    "    \"\"\"\n",
    "    Plot seasonal decomposition\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    decomposition = seasonal_decompose(timeseries, model='additive', period=period)\n",
    "\n",
    "    plt.subplot(4, 1, 1)\n",
    "    decomposition.observed.plot()\n",
    "    plt.title(f'{title} - Original')\n",
    "\n",
    "    plt.subplot(4, 1, 2)\n",
    "    decomposition.trend.plot()\n",
    "    plt.title('Trend')\n",
    "\n",
    "    plt.subplot(4, 1, 3)\n",
    "    decomposition.seasonal.plot()\n",
    "    plt.title('Seasonal')\n",
    "\n",
    "    plt.subplot(4, 1, 4)\n",
    "    decomposition.resid.plot()\n",
    "    plt.title('Residual')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1751907771181,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "PKDiBkAoxF4j"
   },
   "outputs": [],
   "source": [
    "def find_arima_order(timeseries, max_p=3, max_d=2, max_q=3):\n",
    "    \"\"\"\n",
    "    Find optimal ARIMA order using grid search\n",
    "    \"\"\"\n",
    "    print(\"Finding optimal ARIMA parameters...\")\n",
    "\n",
    "    best_aic = float('inf')\n",
    "    best_order = None\n",
    "    results = []\n",
    "\n",
    "    for p in range(max_p + 1):\n",
    "        for d in range(max_d + 1):\n",
    "            for q in range(max_q + 1):\n",
    "                try:\n",
    "                    model = ARIMA(timeseries, order=(p, d, q))\n",
    "                    fitted_model = model.fit()\n",
    "                    aic = fitted_model.aic\n",
    "                    results.append((p, d, q, aic))\n",
    "\n",
    "                    if aic < best_aic:\n",
    "                        best_aic = aic\n",
    "                        best_order = (p, d, q)\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    print(f\"Best ARIMA order: {best_order} with AIC: {best_aic:.2f}\")\n",
    "\n",
    "    # Show top 5 models\n",
    "    results.sort(key=lambda x: x[3])\n",
    "    print(\"\\nTop 5 models:\")\n",
    "    for i, (p, d, q, aic) in enumerate(results[:5]):\n",
    "        print(f\"{i+1}. ARIMA({p},{d},{q}) - AIC: {aic:.2f}\")\n",
    "\n",
    "    return best_order, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1751906938349,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "wvSd4f4SymCg"
   },
   "outputs": [],
   "source": [
    "def weighted_mae(y_true, y_pred, is_holiday):\n",
    "    weights = np.where(is_holiday, 5, 1)\n",
    "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNJGdk4JxQnE"
   },
   "source": [
    "#Department Arima Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1751906791190,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "aameJAH6xWpP"
   },
   "outputs": [],
   "source": [
    "class DepartmentARIMA:\n",
    "    \"\"\"\n",
    "    ARIMA model for a specific department\n",
    "    \"\"\"\n",
    "    def __init__(self, department, order=None):\n",
    "        self.department = department\n",
    "        self.order = order\n",
    "        self.model = None\n",
    "        self.fitted_model = None\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def prepare_data(self, dept_data):\n",
    "        \"\"\"\n",
    "        Prepare time series data for ARIMA\n",
    "        \"\"\"\n",
    "        # Sort by date\n",
    "        dept_data = dept_data.sort_values('Date')\n",
    "\n",
    "        # Use sales mean as target (already aggregated across stores)\n",
    "        self.ts_data = dept_data.set_index('Date')['Sales_Mean']\n",
    "\n",
    "        # Handle missing values\n",
    "        self.ts_data = self.ts_data.fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "        print(f\"Department {self.department} - Data shape: {self.ts_data.shape}\")\n",
    "        print(f\"Date range: {self.ts_data.index.min()} to {self.ts_data.index.max()}\")\n",
    "\n",
    "        return self.ts_data\n",
    "\n",
    "    def fit(self, dept_data, auto_order=True):\n",
    "        \"\"\"\n",
    "        Fit ARIMA model\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Fitting ARIMA for Department {self.department} ===\")\n",
    "\n",
    "        # Prepare data\n",
    "        ts_data = self.prepare_data(dept_data)\n",
    "\n",
    "        # Check stationarity\n",
    "        is_stationary = check_stationarity(ts_data, f\"Department {self.department}\")\n",
    "\n",
    "        # Find optimal order if not specified\n",
    "        if auto_order or self.order is None:\n",
    "            self.order, _ = find_arima_order(ts_data)\n",
    "\n",
    "        # Fit model\n",
    "        try:\n",
    "            self.model = ARIMA(ts_data, order=self.order)\n",
    "            self.fitted_model = self.model.fit()\n",
    "\n",
    "            print(f\"Model fitted successfully!\")\n",
    "            print(f\"AIC: {self.fitted_model.aic:.2f}\")\n",
    "            print(f\"Order: {self.order}\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting model for department {self.department}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def forecast(self, steps):\n",
    "        \"\"\"\n",
    "        Make forecasts\n",
    "        \"\"\"\n",
    "        if self.fitted_model is None:\n",
    "            raise ValueError(\"Model not fitted yet!\")\n",
    "\n",
    "        forecast = self.fitted_model.forecast(steps=steps)\n",
    "        conf_int = self.fitted_model.get_forecast(steps=steps).conf_int()\n",
    "\n",
    "        return forecast, conf_int\n",
    "\n",
    "    def plot_diagnostics(self):\n",
    "        \"\"\"\n",
    "        Plot model diagnostics\n",
    "        \"\"\"\n",
    "        if self.fitted_model is None:\n",
    "            raise ValueError(\"Model not fitted yet!\")\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Residuals\n",
    "        residuals = self.fitted_model.resid\n",
    "        axes[0, 0].plot(residuals)\n",
    "        axes[0, 0].set_title('Residuals')\n",
    "\n",
    "        # Q-Q plot\n",
    "        from scipy import stats\n",
    "        stats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "        axes[0, 1].set_title('Q-Q Plot')\n",
    "\n",
    "        # ACF of residuals\n",
    "        plot_acf(residuals, ax=axes[1, 0], lags=20)\n",
    "        axes[1, 0].set_title('ACF of Residuals')\n",
    "\n",
    "        # PACF of residuals\n",
    "        plot_pacf(residuals, ax=axes[1, 1], lags=20)\n",
    "        axes[1, 1].set_title('PACF of Residuals')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUV4vb3wyIX7"
   },
   "source": [
    "#Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1751907039803,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "zainmnEDyLTa"
   },
   "outputs": [],
   "source": [
    "def train_department_models(dept_features_df, top_departments=None, n_depts=5):\n",
    "    \"\"\"\n",
    "    Train ARIMA models for multiple departments\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Training Department Models ===\")\n",
    "\n",
    "    if top_departments is None:\n",
    "        # Select top N departments by total sales\n",
    "        dept_totals = dept_features_df.groupby('Dept')['Sales_Sum'].sum().sort_values(ascending=False)\n",
    "        top_departments = dept_totals.head(n_depts).index.tolist()\n",
    "\n",
    "    print(f\"Training models for departments: {top_departments}\")\n",
    "\n",
    "    models = {}\n",
    "    results = []\n",
    "\n",
    "    for dept in top_departments:\n",
    "        print(f\"\\nTraining model for Department {dept}...\")\n",
    "\n",
    "        # Get department data\n",
    "        dept_data = dept_features_df[dept_features_df['Dept'] == dept].copy()\n",
    "\n",
    "        if len(dept_data) < 20:  # Need minimum data points\n",
    "            print(f\"Skipping department {dept} - insufficient data ({len(dept_data)} points)\")\n",
    "            continue\n",
    "\n",
    "        # Start MLflow run for this department\n",
    "        with mlflow.start_run(run_name=f\"arima_dept_{dept}\"):\n",
    "\n",
    "            # Initialize and train model\n",
    "            model = DepartmentARIMA(dept)\n",
    "            success = model.fit(dept_data)\n",
    "\n",
    "            if success:\n",
    "                models[dept] = model\n",
    "\n",
    "                # Log parameters\n",
    "                mlflow.log_param(\"department\", dept)\n",
    "                mlflow.log_param(\"arima_order\", model.order)\n",
    "                mlflow.log_param(\"data_points\", len(dept_data))\n",
    "\n",
    "                # Log metrics\n",
    "                mlflow.log_metric(\"aic\", model.fitted_model.aic)\n",
    "                mlflow.log_metric(\"bic\", model.fitted_model.bic)\n",
    "\n",
    "                # Calculate in-sample metrics\n",
    "                fitted_values = model.fitted_model.fittedvalues\n",
    "                actual_values = model.ts_data\n",
    "\n",
    "                # Align the series (fitted values might be shorter due to differencing)\n",
    "                min_len = min(len(fitted_values), len(actual_values))\n",
    "                fitted_aligned = fitted_values[-min_len:]\n",
    "                actual_aligned = actual_values[-min_len:]\n",
    "\n",
    "                mae = mean_absolute_error(actual_aligned, fitted_aligned)\n",
    "                rmse = np.sqrt(mean_squared_error(actual_aligned, fitted_aligned))\n",
    "                # wmae = weighted_mae(actual_aligned, fitted_aligned, h_val.values) need holiday value here to add wmae\n",
    "\n",
    "                mlflow.log_metric(\"mae\", mae)\n",
    "                mlflow.log_metric(\"rmse\", rmse)\n",
    "                # mlflow.log_metric(\"wmae\", wmae)\n",
    "\n",
    "                # Save model summary\n",
    "                model_summary = str(model.fitted_model.summary())\n",
    "                mlflow.log_text(model_summary, \"model_summary.txt\")\n",
    "\n",
    "                results.append({\n",
    "                    'Department': dept,\n",
    "                    'Order': model.order,\n",
    "                    'AIC': model.fitted_model.aic,\n",
    "                    'MAE': mae,\n",
    "                    'RMSE': rmse,\n",
    "                    'Data_Points': len(dept_data)\n",
    "                    # 'WMAE': wmae\n",
    "                })\n",
    "\n",
    "                print(f\"Department {dept} - AIC: {model.fitted_model.aic:.2f}, MAE: {mae:.2f}\")\n",
    "\n",
    "    # Create results summary\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n=== Training Results Summary ===\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    return models, results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRbJx8Hkzhxy"
   },
   "source": [
    "#Forecasting and Generating submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1751908680832,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "eSiiUr0ty8AX"
   },
   "outputs": [],
   "source": [
    "def generate_forecasts(models, test_df, submission_weeks=39):\n",
    "    \"\"\"\n",
    "    Generate forecasts for test period\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Generating Forecasts ===\")\n",
    "\n",
    "    forecasts = []\n",
    "\n",
    "    for dept, model in models.items():\n",
    "        print(f\"Forecasting for Department {dept}...\")\n",
    "\n",
    "        # Get test data for this department\n",
    "        test_df[\"Id\"] = test_df[\"Store\"].astype(str) + \"_\" + test_df[\"Dept\"].astype(str) + \"_\" + test_df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        dept_test = test_df[test_df['Dept'] == dept].copy()\n",
    "\n",
    "        if len(dept_test) == 0:\n",
    "            print(f\"No test data for department {dept}\")\n",
    "            continue\n",
    "\n",
    "        # Generate forecasts\n",
    "        forecast, conf_int = model.forecast(steps=submission_weeks)\n",
    "\n",
    "        # Create submission format\n",
    "        # We need to replicate forecasts for each store in the department\n",
    "        stores_in_dept = dept_test['Store'].unique()\n",
    "\n",
    "        for store in stores_in_dept:\n",
    "            store_test = dept_test[dept_test['Store'] == store].copy()\n",
    "            store_test = store_test.sort_values('Date')\n",
    "\n",
    "            # Assign forecasts to weeks\n",
    "            for i, (idx, row) in enumerate(store_test.iterrows()):\n",
    "                if i < len(forecast):\n",
    "                    forecasts.append({\n",
    "                        'Id': row['Id'],\n",
    "                        'Weekly_Sales': forecast.iloc[i]\n",
    "                    })\n",
    "                else:\n",
    "                    # Use last forecast value if we run out\n",
    "                    forecasts.append({\n",
    "                        'Id': row['Id'],\n",
    "                        'Weekly_Sales': forecast.iloc[-1]\n",
    "                    })\n",
    "\n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame(forecasts)\n",
    "\n",
    "    print(f\"Generated {len(submission_df)} forecasts\")\n",
    "    print(f\"Departments covered: {len(models)}\")\n",
    "\n",
    "    return submission_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSMSqNMD0KGP"
   },
   "source": [
    "#Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1751907584364,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "a-ltSTDa0LP_"
   },
   "outputs": [],
   "source": [
    "def main_pipeline():\n",
    "    \"\"\"\n",
    "    Main execution pipeline\n",
    "    \"\"\"\n",
    "    print(\"=== WALMART SALES FORECASTING - ARIMA BASELINE ===\")\n",
    "    print(\"Date:\", pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # STEP 1: Load and explore data\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 1: DATA LOADING AND EXPLORATION\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    TRAIN_PATH = \"data/train.csv\"\n",
    "    TEST_PATH = \"data/test.csv\"\n",
    "    FEATURES_PATH = \"data/features.csv\"\n",
    "    STORES_PATH = \"data/stores.csv\"\n",
    "\n",
    "    try:\n",
    "        train_df, test_df, features_df, stores_df = load_and_explore_data(\n",
    "            TRAIN_PATH, TEST_PATH, FEATURES_PATH, STORES_PATH\n",
    "        )\n",
    "\n",
    "        # Explore department patterns\n",
    "        dept_sales, top_depts = explore_department_patterns(train_df)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure all CSV files are in the correct location\")\n",
    "        return\n",
    "\n",
    "    # STEP 2: Feature engineering\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 2: FEATURE ENGINEERING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    dept_features_df = create_features(train_df, features_df, stores_df)\n",
    "\n",
    "    # STEP 3: Model training\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 3: MODEL TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Train models for top 5 departments\n",
    "    models, results_df = train_department_models(dept_features_df, n_depts=5)\n",
    "\n",
    "    # STEP 4: Generate forecasts\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 4: FORECASTING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if len(models) > 0:\n",
    "        submission_df = generate_forecasts(models, test_df)\n",
    "\n",
    "        # Save submission\n",
    "        submission_df.to_csv('submission_arima_baseline.csv', index=False)\n",
    "        print(\"Submission saved as 'submission_arima_baseline.csv'\")\n",
    "\n",
    "        # Log final results\n",
    "        with mlflow.start_run(run_name=\"final_results\"):\n",
    "            mlflow.log_metric(\"departments_modeled\", len(models))\n",
    "            mlflow.log_metric(\"forecasts_generated\", len(submission_df))\n",
    "            mlflow.log_artifact(\"submission_arima_baseline.csv\")\n",
    "\n",
    "            # Log results summary\n",
    "            results_df.to_csv(\"training_results.csv\", index=False)\n",
    "            mlflow.log_artifact(\"training_results.csv\")\n",
    "\n",
    "    else:\n",
    "        print(\"No models were successfully trained!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PIPELINE COMPLETED\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Check MLflow UI at: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 129558,
     "status": "ok",
     "timestamp": 1751908821807,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "z-fN6Jd41ESi",
    "outputId": "8ada79ea-8240-4321-9c3b-bf1323fce2f9"
   },
   "outputs": [],
   "source": [
    "main_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StYhoiy14UhT"
   },
   "source": [
    "#Additional Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1751908481007,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "9T4vdw1x4WTT"
   },
   "outputs": [],
   "source": [
    "def analyze_single_department(dept_features_df, department, plot=True):\n",
    "    \"\"\"\n",
    "    Detailed analysis of a single department\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Detailed Analysis: Department {department} ===\")\n",
    "\n",
    "    dept_data = dept_features_df[dept_features_df['Dept'] == department].copy()\n",
    "    dept_data = dept_data.sort_values('Date')\n",
    "\n",
    "    if len(dept_data) == 0:\n",
    "        print(f\"No data found for department {department}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Data points: {len(dept_data)}\")\n",
    "    print(f\"Date range: {dept_data['Date'].min()} to {dept_data['Date'].max()}\")\n",
    "    print(f\"Sales statistics:\")\n",
    "    print(dept_data['Sales_Mean'].describe())\n",
    "\n",
    "    if plot:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Time series plot\n",
    "        axes[0, 0].plot(dept_data['Date'], dept_data['Sales_Mean'])\n",
    "        axes[0, 0].set_title(f'Department {department} - Sales Time Series')\n",
    "        axes[0, 0].set_xlabel('Date')\n",
    "        axes[0, 0].set_ylabel('Average Sales')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Seasonal decomposition\n",
    "        ts_data = dept_data.set_index('Date')['Sales_Mean']\n",
    "        decomposition = seasonal_decompose(ts_data, model='additive', period=52)\n",
    "\n",
    "        axes[0, 1].plot(decomposition.seasonal)\n",
    "        axes[0, 1].set_title('Seasonal Component')\n",
    "\n",
    "        # ACF and PACF\n",
    "        plot_acf(ts_data, ax=axes[1, 0], lags=20)\n",
    "        plot_pacf(ts_data, ax=axes[1, 1], lags=20)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return dept_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1751908479247,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "t2bC3IL64ZvU"
   },
   "outputs": [],
   "source": [
    "def compare_department_performance(results_df):\n",
    "    \"\"\"\n",
    "    Compare performance across departments\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Department Performance Comparison ===\")\n",
    "\n",
    "    # Sort by AIC\n",
    "    results_sorted = results_df.sort_values('AIC')\n",
    "\n",
    "    print(\"Best performing departments (by AIC):\")\n",
    "    print(results_sorted[['Department', 'AIC', 'MAE', 'RMSE']].to_string(index=False))\n",
    "\n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # AIC comparison\n",
    "    axes[0].bar(results_sorted['Department'].astype(str), results_sorted['AIC'])\n",
    "    axes[0].set_title('AIC by Department')\n",
    "    axes[0].set_xlabel('Department')\n",
    "    axes[0].set_ylabel('AIC')\n",
    "\n",
    "    # MAE comparison\n",
    "    axes[1].bar(results_sorted['Department'].astype(str), results_sorted['MAE'])\n",
    "    axes[1].set_title('MAE by Department')\n",
    "    axes[1].set_xlabel('Department')\n",
    "    axes[1].set_ylabel('MAE')\n",
    "\n",
    "    # RMSE comparison\n",
    "    axes[2].bar(results_sorted['Department'].astype(str), results_sorted['RMSE'])\n",
    "    axes[2].set_title('RMSE by Department')\n",
    "    axes[2].set_xlabel('Department')\n",
    "    axes[2].set_ylabel('RMSE')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZJTlabQFX6g255B4N7mFi",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
