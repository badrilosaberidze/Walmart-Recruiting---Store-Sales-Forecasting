{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMY2DlO9s+9tQhtXIm70gNU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Env Setup"],"metadata":{"id":"T6fYcMmd_vka"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DIXh9Diw_cvn","executionInfo":{"status":"ok","timestamp":1751995196394,"user_tz":-240,"elapsed":29050,"user":{"displayName":"Badri Losaberidze","userId":"06913732304764740270"}},"outputId":"f813754e-8fb4-4ecd-aec4-1781e7dc40b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","\n","with open('/content/drive/MyDrive/MLFinal/git_token.env', 'r') as f:\n","    token = f.read().strip()\n","\n","username = \"badrilosaberidze\"\n","\n","%cd /content/drive/MyDrive/MLFinal/walmart-sales-forecasting\n","!git remote set-url origin https://{username}:{token}@github.com/{username}/Walmart-Recruiting---Store-Sales-Forecasting.git\n","!git pull"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zpS_i3TS_u6Z","executionInfo":{"status":"ok","timestamp":1751995219387,"user_tz":-240,"elapsed":22997,"user":{"displayName":"Badri Losaberidze","userId":"06913732304764740270"}},"outputId":"22d08652-1d20-4941-e144-92e82ca70f5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MLFinal/walmart-sales-forecasting\n","Already up to date.\n"]}]},{"cell_type":"code","source":["import xgboost as xgb\n","from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n","from sklearn.metrics import make_scorer\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.preprocessing import LabelEncoder, TargetEncoder\n","from sklearn.metrics import mean_absolute_error\n","import wandb"],"metadata":{"id":"TKsiLfn6_6uj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Custom WMAE Functions"],"metadata":{"id":"vctgtVI_ACeV"}},{"cell_type":"code","source":["def calculate_wmae(y_true, y_pred, is_holiday):\n","    \"\"\"\n","    Calculate Weighted Mean Absolute Error (WMAE)\n","    Holiday weeks have 5x weight\n","    \"\"\"\n","    weights = np.where(is_holiday, 5.0, 1.0)\n","    weighted_errors = weights * np.abs(y_true - y_pred)\n","    wmae = np.sum(weighted_errors) / np.sum(weights)\n","    return wmae"],"metadata":{"id":"2ZQDq3ERAEiC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Optimized XGBoost Model"],"metadata":{"id":"yRw7QogqAMbN"}},{"cell_type":"code","source":["class OptimizedWalmartXGBoost:\n","    \"\"\"\n","    Optimized XGBoost with better features and hyperparameter tuning\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.model = None\n","        self.best_params = None\n","        self.feature_names = None\n","        self.target_encoders = {}\n","        self.feature_importance = None\n","        self.metrics = None\n","        self.scaler_stats = {}\n","\n","    def create_optimized_features(self, df, features_df, stores_df, is_training=True):\n","        \"\"\"\n","        Enhanced feature engineering with domain knowledge\n","        \"\"\"\n","        print(\"üîß Creating optimized features...\")\n","\n","        data = df.copy()\n","        data = data.merge(features_df, on=['Store', 'Date'], how='left')\n","        data = data.merge(stores_df, on='Store', how='left')\n","        data['Date'] = pd.to_datetime(data['Date'])\n","\n","        # Sort for time-based features\n","        data = data.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n","\n","        # üìÖ ENHANCED TIME FEATURES\n","        data['Year'] = data['Date'].dt.year\n","        data['Month'] = data['Date'].dt.month\n","        data['Week'] = data['Date'].dt.isocalendar().week\n","        data['Quarter'] = data['Date'].dt.quarter\n","        data['DayOfYear'] = data['Date'].dt.dayofyear\n","        data['WeekOfMonth'] = data['Date'].dt.day // 7 + 1\n","        data['IsYearEnd'] = (data['Month'] == 12).astype(int)\n","        data['IsYearStart'] = (data['Month'] == 1).astype(int)\n","\n","        # üéÑ COMPREHENSIVE HOLIDAY FEATURES\n","        holiday_col = 'IsHoliday_x' if 'IsHoliday_x' in data.columns else 'IsHoliday'\n","        if holiday_col in data.columns:\n","            data['IsHoliday'] = data[holiday_col].astype(int)\n","        else:\n","            data['IsHoliday'] = 0\n","\n","        # Specific holiday seasons\n","        data['Christmas_Season'] = ((data['Month'] == 12) & (data['Week'] >= 48)).astype(int)\n","        data['BlackFriday_Week'] = ((data['Month'] == 11) & (data['Week'] == 47)).astype(int)\n","        data['Thanksgiving_Week'] = ((data['Month'] == 11) & (data['Week'] >= 46)).astype(int)\n","        data['BackToSchool'] = ((data['Month'].isin([7, 8, 9]))).astype(int)\n","        data['Easter_Season'] = ((data['Month'].isin([3, 4]))).astype(int)\n","\n","        # Holiday interactions\n","        data['Holiday_Month'] = data['IsHoliday'] * data['Month']\n","        data['Holiday_Week'] = data['IsHoliday'] * data['Week']\n","        data['Holiday_Quarter'] = data['IsHoliday'] * data['Quarter']\n","\n","        # Pre/post holiday effects\n","        data['PreHoliday_2w'] = data.groupby(['Store', 'Dept'])['IsHoliday'].shift(-2).fillna(0)\n","        data['PreHoliday_1w'] = data.groupby(['Store', 'Dept'])['IsHoliday'].shift(-1).fillna(0)\n","        data['PostHoliday_1w'] = data.groupby(['Store', 'Dept'])['IsHoliday'].shift(1).fillna(0)\n","        data['PostHoliday_2w'] = data.groupby(['Store', 'Dept'])['IsHoliday'].shift(2).fillna(0)\n","\n","        # üè™ ENHANCED STORE FEATURES\n","        # Store size categories\n","        if 'Size' in data.columns:\n","            data['Size'] = data['Size'].fillna(data['Size'].median())\n","            data['Size_Category'] = pd.qcut(data['Size'], q=5, labels=False, duplicates='drop')\n","            data['Size_Large'] = (data['Size'] > data['Size'].quantile(0.8)).astype(int)\n","            data['Size_Small'] = (data['Size'] < data['Size'].quantile(0.2)).astype(int)\n","\n","        # Store type enhanced\n","        data['Type'] = data['Type'].fillna('A')  # Most common type\n","        data['Type_A'] = (data['Type'] == 'A').astype(int)\n","        data['Type_B'] = (data['Type'] == 'B').astype(int)\n","        data['Type_C'] = (data['Type'] == 'C').astype(int)\n","\n","        # üìä DEPARTMENT CATEGORIES (domain knowledge)\n","        # Group departments by business logic\n","        grocery_depts = [1, 3, 6, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28]\n","        clothing_depts = [2, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 26]\n","        electronics_depts = [72, 87, 88]\n","        seasonal_depts = [67, 78, 79, 80, 85, 95, 96, 97, 98, 99]\n","\n","        data['Dept_Grocery'] = data['Dept'].isin(grocery_depts).astype(int)\n","        data['Dept_Clothing'] = data['Dept'].isin(clothing_depts).astype(int)\n","        data['Dept_Electronics'] = data['Dept'].isin(electronics_depts).astype(int)\n","        data['Dept_Seasonal'] = data['Dept'].isin(seasonal_depts).astype(int)\n","\n","        # üå°Ô∏è ENHANCED EXTERNAL FEATURES\n","        numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n","        for col in numeric_cols:\n","            if col in data.columns:\n","                data[col] = data[col].fillna(data[col].median())\n","\n","                # Binning for non-linear relationships\n","                data[f'{col}_High'] = (data[col] > data[col].quantile(0.8)).astype(int)\n","                data[f'{col}_Low'] = (data[col] < data[col].quantile(0.2)).astype(int)\n","\n","                # Interactions with holidays and seasons\n","                data[f'{col}_x_Holiday'] = data[col] * data['IsHoliday']\n","                data[f'{col}_x_Christmas'] = data[col] * data['Christmas_Season']\n","\n","                # Moving averages (if we have enough data)\n","                if len(data) > 4:\n","                    data[f'{col}_MA4'] = data[col].rolling(window=4, min_periods=1).mean()\n","\n","        # Weather-specific features\n","        if 'Temperature' in data.columns:\n","            data['Cold_Weather'] = (data['Temperature'] < 32).astype(int)  # Freezing\n","            data['Hot_Weather'] = (data['Temperature'] > 80).astype(int)  # Hot\n","            data['Mild_Weather'] = ((data['Temperature'] >= 60) & (data['Temperature'] <= 75)).astype(int)\n","\n","        # üìà ADVANCED INTERACTIONS\n","        # Store-Department combinations (high cardinality)\n","        data['Store_x_Dept'] = data['Store'] * 1000 + data['Dept']  # Unique combo\n","\n","        # Time-Department interactions\n","        data['Month_x_Dept'] = data['Month'] * 100 + data['Dept']\n","        data['Quarter_x_Dept'] = data['Quarter'] * 100 + data['Dept']\n","        data['Week_x_Type'] = data['Week'] * 10 + data['Type'].astype('category').cat.codes\n","\n","        # Holiday-Department interactions (crucial)\n","        data['Christmas_x_Grocery'] = data['Christmas_Season'] * data['Dept_Grocery']\n","        data['Christmas_x_Electronics'] = data['Christmas_Season'] * data['Dept_Electronics']\n","        data['Holiday_x_Seasonal'] = data['IsHoliday'] * data['Dept_Seasonal']\n","\n","        # Size-based interactions\n","        if 'Size' in data.columns:\n","            data['Size_x_Holiday'] = data['Size'] * data['IsHoliday']\n","            data['Size_x_Christmas'] = data['Size'] * data['Christmas_Season']\n","            data['Large_Store_Holiday'] = data['Size_Large'] * data['IsHoliday']\n","\n","        # üîÑ SEASONAL ENCODING (multiple frequencies)\n","        # Monthly patterns (12-month cycle)\n","        data['Month_sin'] = np.sin(2 * np.pi * data['Month'] / 12)\n","        data['Month_cos'] = np.cos(2 * np.pi * data['Month'] / 12)\n","\n","        # Weekly patterns (52-week cycle)\n","        data['Week_sin'] = np.sin(2 * np.pi * data['Week'] / 52)\n","        data['Week_cos'] = np.cos(2 * np.pi * data['Week'] / 52)\n","\n","        # Quarterly patterns (4-quarter cycle)\n","        data['Quarter_sin'] = np.sin(2 * np.pi * data['Quarter'] / 4)\n","        data['Quarter_cos'] = np.cos(2 * np.pi * data['Quarter'] / 4)\n","\n","        # Bi-annual pattern (business cycles)\n","        data['Biannual_sin'] = np.sin(2 * np.pi * data['Month'] / 6)\n","        data['Biannual_cos'] = np.cos(2 * np.pi * data['Month'] / 6)\n","\n","        # Categorical encoding\n","        data['Store'] = data['Store'].astype('category').cat.codes\n","        data['Dept'] = data['Dept'].astype('category').cat.codes\n","        data['Type'] = data['Type'].astype('category').cat.codes\n","\n","        # Fill any remaining NaN\n","        data = data.fillna(0)\n","\n","        print(f\"‚úÖ Optimized feature engineering complete. Shape: {data.shape}\")\n","        print(f\"üìä Features created: {data.shape[1]} total columns\")\n","\n","        return data\n","\n","    def prepare_data_optimized(self, train_df, features_df, stores_df, test_df=None):\n","        \"\"\"\n","        Optimized data preparation with proper target encoding\n","        \"\"\"\n","        print(\"üìä Preparing optimized data...\")\n","\n","        # Create features\n","        train_features = self.create_optimized_features(train_df, features_df, stores_df, is_training=True)\n","\n","        # Time-aware split for target encoding\n","        split_idx = int(len(train_features) * 0.8)\n","        train_part = train_features.iloc[:split_idx].copy()\n","        val_part = train_features.iloc[split_idx:].copy()\n","\n","        # TARGET ENCODING (no data leakage)\n","        if 'Weekly_Sales' in train_part.columns:\n","            print(\"üìä Computing advanced target encodings...\")\n","\n","            # Store encodings\n","            store_means = train_part.groupby('Store')['Weekly_Sales'].mean()\n","            store_stds = train_part.groupby('Store')['Weekly_Sales'].std()\n","\n","            # Department encodings\n","            dept_means = train_part.groupby('Dept')['Weekly_Sales'].mean()\n","            dept_stds = train_part.groupby('Dept')['Weekly_Sales'].std()\n","\n","            # Store-Department interaction\n","            store_dept_means = train_part.groupby(['Store', 'Dept'])['Weekly_Sales'].mean()\n","\n","            # Store-Month seasonality\n","            store_month_means = train_part.groupby(['Store', 'Month'])['Weekly_Sales'].mean()\n","\n","            # Department-Month seasonality\n","            dept_month_means = train_part.groupby(['Dept', 'Month'])['Weekly_Sales'].mean()\n","\n","            # Apply to train part\n","            train_part['Store_TargetEnc'] = train_part['Store'].map(store_means).fillna(store_means.mean())\n","            train_part['Store_TargetStd'] = train_part['Store'].map(store_stds).fillna(store_stds.mean())\n","            train_part['Dept_TargetEnc'] = train_part['Dept'].map(dept_means).fillna(dept_means.mean())\n","            train_part['Dept_TargetStd'] = train_part['Dept'].map(dept_stds).fillna(dept_stds.mean())\n","            train_part['StoreDept_TargetEnc'] = train_part.set_index(['Store', 'Dept']).index.map(store_dept_means).fillna(0)\n","            train_part['StoreMonth_TargetEnc'] = train_part.set_index(['Store', 'Month']).index.map(store_month_means).fillna(0)\n","            train_part['DeptMonth_TargetEnc'] = train_part.set_index(['Dept', 'Month']).index.map(dept_month_means).fillna(0)\n","\n","            # Apply to validation part\n","            val_part['Store_TargetEnc'] = val_part['Store'].map(store_means).fillna(store_means.mean())\n","            val_part['Store_TargetStd'] = val_part['Store'].map(store_stds).fillna(store_stds.mean())\n","            val_part['Dept_TargetEnc'] = val_part['Dept'].map(dept_means).fillna(dept_means.mean())\n","            val_part['Dept_TargetStd'] = val_part['Dept'].map(dept_stds).fillna(dept_stds.mean())\n","            val_part['StoreDept_TargetEnc'] = val_part.set_index(['Store', 'Dept']).index.map(store_dept_means).fillna(0)\n","            val_part['StoreMonth_TargetEnc'] = val_part.set_index(['Store', 'Month']).index.map(store_month_means).fillna(0)\n","            val_part['DeptMonth_TargetEnc'] = val_part.set_index(['Dept', 'Month']).index.map(dept_month_means).fillna(0)\n","\n","            # Store encoders\n","            self.target_encoders = {\n","                'Store': store_means, 'Store_std': store_stds,\n","                'Dept': dept_means, 'Dept_std': dept_stds,\n","                'StoreDept': store_dept_means,\n","                'StoreMonth': store_month_means,\n","                'DeptMonth': dept_month_means\n","            }\n","\n","            # Combine back\n","            train_features_final = pd.concat([train_part, val_part], ignore_index=True)\n","\n","        # Feature selection\n","        exclude_cols = ['Weekly_Sales', 'Date', 'Id']\n","        feature_cols = [col for col in train_features_final.columns if col not in exclude_cols]\n","\n","        X_train = train_features_final[feature_cols]\n","        y_train = train_features_final['Weekly_Sales']\n","\n","        self.feature_names = feature_cols\n","\n","        # Prepare test data\n","        X_test = None\n","        if test_df is not None:\n","            test_features = self.create_optimized_features(test_df, features_df, stores_df, is_training=False)\n","\n","            # Apply stored target encodings\n","            if self.target_encoders:\n","                test_features['Store_TargetEnc'] = test_features['Store'].map(self.target_encoders['Store']).fillna(self.target_encoders['Store'].mean())\n","                test_features['Store_TargetStd'] = test_features['Store'].map(self.target_encoders['Store_std']).fillna(self.target_encoders['Store_std'].mean())\n","                test_features['Dept_TargetEnc'] = test_features['Dept'].map(self.target_encoders['Dept']).fillna(self.target_encoders['Dept'].mean())\n","                test_features['Dept_TargetStd'] = test_features['Dept'].map(self.target_encoders['Dept_std']).fillna(self.target_encoders['Dept_std'].mean())\n","                test_features['StoreDept_TargetEnc'] = test_features.set_index(['Store', 'Dept']).index.map(self.target_encoders['StoreDept']).fillna(0)\n","                test_features['StoreMonth_TargetEnc'] = test_features.set_index(['Store', 'Month']).index.map(self.target_encoders['StoreMonth']).fillna(0)\n","                test_features['DeptMonth_TargetEnc'] = test_features.set_index(['Dept', 'Month']).index.map(self.target_encoders['DeptMonth']).fillna(0)\n","            else:\n","                # Add dummy encodings\n","                encoding_cols = ['Store_TargetEnc', 'Store_TargetStd', 'Dept_TargetEnc', 'Dept_TargetStd',\n","                               'StoreDept_TargetEnc', 'StoreMonth_TargetEnc', 'DeptMonth_TargetEnc']\n","                for col in encoding_cols:\n","                    test_features[col] = 0\n","\n","            X_test = test_features[feature_cols]\n","\n","        print(f\"‚úÖ Optimized data prepared - Train: {X_train.shape}, Features: {len(feature_cols)}\")\n","        if X_test is not None:\n","            print(f\"‚úÖ Test data: {X_test.shape}\")\n","\n","        return X_train, y_train, X_test\n","\n","    def optimize_hyperparameters(self, X_train, y_train):\n","        \"\"\"\n","        GridSearch for optimal hyperparameters\n","        \"\"\"\n","        print(\"üîç Starting hyperparameter optimization...\")\n","\n","        # Custom WMAE scorer for GridSearch\n","        def wmae_scorer(y_true, y_pred):\n","            # Simple approximation: use 10% holiday rate\n","            is_holiday = np.random.random(len(y_true)) < 0.1\n","            return -calculate_wmae(y_true, y_pred, is_holiday)  # Negative because sklearn maximizes\n","\n","        wmae_score = make_scorer(wmae_scorer, greater_is_better=True)\n","\n","        # Time series cross-validation\n","        tscv = TimeSeriesSplit(n_splits=3)\n","\n","        # Parameter grid (focused on most important params)\n","        param_grid = {\n","            'max_depth': [6, 8, 10],\n","            'learning_rate': [0.03, 0.05, 0.1],\n","            'n_estimators': [800, 1200, 1600],\n","            'subsample': [0.8, 0.9],\n","            'colsample_bytree': [0.8, 0.9],\n","            'reg_alpha': [0, 0.1],\n","            'reg_lambda': [1, 1.5]\n","        }\n","\n","        # Base model\n","        base_model = xgb.XGBRegressor(\n","            objective='reg:squarederror',\n","            random_state=42,\n","            n_jobs=-1,\n","            verbosity=0\n","        )\n","\n","        # GridSearch\n","        print(f\"üîç Testing {len(param_grid['max_depth']) * len(param_grid['learning_rate']) * len(param_grid['n_estimators'])} parameter combinations...\")\n","\n","        grid_search = GridSearchCV(\n","            base_model,\n","            param_grid,\n","            cv=tscv,\n","            scoring='neg_mean_absolute_error',\n","            n_jobs=-1,\n","            verbose=1\n","        )\n","\n","        # Fit GridSearch\n","        grid_search.fit(X_train, y_train)\n","\n","        self.best_params = grid_search.best_params_\n","        self.model = grid_search.best_estimator_\n","\n","        print(f\"‚úÖ Hyperparameter optimization complete!\")\n","        print(f\"üèÜ Best parameters: {self.best_params}\")\n","        print(f\"üéØ Best CV score: {-grid_search.best_score_:.2f}\")\n","\n","        return grid_search.best_score_\n","\n","    def train_optimized(self, X_train, y_train, validation_split=0.2):\n","        \"\"\"\n","        Train with optimized hyperparameters\n","        \"\"\"\n","        print(\"üöÄ Training optimized XGBoost...\")\n","\n","        # Optimize hyperparameters first\n","        self.optimize_hyperparameters(X_train, y_train)\n","\n","        # Time series split for final evaluation\n","        split_idx = int(len(X_train) * (1 - validation_split))\n","\n","        X_tr = X_train.iloc[:split_idx]\n","        X_val = X_train.iloc[split_idx:]\n","        y_tr = y_train.iloc[:split_idx]\n","        y_val = y_train.iloc[split_idx:]\n","\n","        # Get holiday flags\n","        is_holiday_train = X_tr['IsHoliday'].values.astype(bool)\n","        is_holiday_val = X_val['IsHoliday'].values.astype(bool)\n","\n","        # Train final model on full training data\n","        self.model.fit(X_train, y_train)\n","\n","        # Evaluate\n","        train_pred = self.model.predict(X_tr)\n","        val_pred = self.model.predict(X_val)\n","\n","        train_mae = mean_absolute_error(y_tr, train_pred)\n","        val_mae = mean_absolute_error(y_val, val_pred)\n","        train_wmae = calculate_wmae(y_tr, train_pred, is_holiday_train)\n","        val_wmae = calculate_wmae(y_val, val_pred, is_holiday_val)\n","\n","        # Feature importance\n","        self.feature_importance = pd.DataFrame({\n","            'feature': X_train.columns,\n","            'importance': self.model.feature_importances_\n","        }).sort_values('importance', ascending=False)\n","\n","        self.metrics = {\n","            'train_mae': train_mae,\n","            'val_mae': val_mae,\n","            'train_wmae': train_wmae,\n","            'val_wmae': val_wmae,\n","            'best_params': self.best_params\n","        }\n","\n","        print(f\"‚úÖ Optimized training complete!\")\n","        print(f\"   Training MAE: {train_mae:.2f}\")\n","        print(f\"   Validation MAE: {val_mae:.2f}\")\n","        print(f\"   Training WMAE: {train_wmae:.2f}\")\n","        print(f\"   Validation WMAE: {val_wmae:.2f}\")\n","\n","        return self.metrics\n","\n","    def predict(self, X_test):\n","        \"\"\"Generate predictions\"\"\"\n","        if self.model is None:\n","            raise ValueError(\"Model not trained yet!\")\n","\n","        predictions = self.model.predict(X_test)\n","        return np.maximum(0, predictions)\n","\n","    def get_feature_importance(self, top_n=30):\n","        \"\"\"Get feature importance\"\"\"\n","        return self.feature_importance.head(top_n) if self.feature_importance is not None else None"],"metadata":{"id":"VaAMZRbxAOgq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Full Pipeline For Optimized XGBoost model"],"metadata":{"id":"rYjEXFNTApuW"}},{"cell_type":"code","source":["def complete_optimized_xgboost_pipeline(train_path, test_path, features_path, stores_path):\n","    \"\"\"\n","    Complete optimized XGBoost pipeline with GridSearch\n","    \"\"\"\n","    print(\"=\"*80)\n","    print(\"WALMART SALES FORECASTING - OPTIMIZED XGBOOST WITH GRIDSEARCH\")\n","    print(\"=\"*80)\n","\n","    wandb.init(\n","        project=\"walmart-forecasting_XGBoost\",\n","        name=f\"xgboost-optimized-{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}\",\n","        tags=[\"xgboost\", \"optimized\", \"gridsearch\", \"advanced-features\"]\n","    )\n","\n","    try:\n","        # Load data\n","        print(\"üìÇ Loading data...\")\n","        train_df = pd.read_csv(train_path)\n","        test_df = pd.read_csv(test_path)\n","        features_df = pd.read_csv(features_path)\n","        stores_df = pd.read_csv(stores_path)\n","\n","        # Convert dates\n","        train_df['Date'] = pd.to_datetime(train_df['Date'])\n","        test_df['Date'] = pd.to_datetime(test_df['Date'])\n","        features_df['Date'] = pd.to_datetime(features_df['Date'])\n","\n","        # Create submission ID\n","        test_df['Id'] = (test_df['Store'].astype(str) + '_' +\n","                        test_df['Dept'].astype(str) + '_' +\n","                        test_df['Date'].dt.strftime('%Y-%m-%d'))\n","\n","        # Log config\n","        wandb.config.update({\n","            'model_type': 'XGBoost Optimized',\n","            'hyperparameter_tuning': 'GridSearchCV',\n","            'features': 'advanced_domain_knowledge',\n","            'target_encoding': 'advanced_multi_level',\n","            'train_rows': len(train_df)\n","        })\n","\n","        # Initialize and train\n","        model = OptimizedWalmartXGBoost()\n","        X_train, y_train, X_test = model.prepare_data_optimized(train_df, features_df, stores_df, test_df)\n","\n","        # Train with optimization\n","        metrics = model.train_optimized(X_train, y_train)\n","\n","        # Log metrics\n","        wandb.log({\n","            'train_mae': metrics['train_mae'],\n","            'val_mae': metrics['val_mae'],\n","            'train_wmae': metrics['train_wmae'],\n","            'val_wmae': metrics['val_wmae'],\n","            'features_count': len(model.feature_names)\n","        })\n","\n","        # Log best parameters\n","        wandb.log({\"best_hyperparameters\": metrics['best_params']})\n","\n","        # Feature importance\n","        importance_df = model.get_feature_importance(30)\n","        print(\"\\nüîç Top 20 Feature Importances:\")\n","        print(importance_df.head(20))\n","\n","        importance_table = wandb.Table(dataframe=importance_df)\n","        wandb.log({\"feature_importance\": importance_table})\n","\n","        # Generate predictions\n","        print(\"üîÆ Generating optimized predictions...\")\n","        predictions = model.predict(X_test)\n","\n","        # Create submission\n","        submission_df = pd.DataFrame({\n","            'Id': test_df['Id'],\n","            'Weekly_Sales': predictions\n","        })\n","\n","        # Save results\n","        submission_filename = 'walmart_xgboost_optimized_submission.csv'\n","        submission_df.to_csv(submission_filename, index=False)\n","\n","        # Log submission stats\n","        wandb.log({\n","            'submission_total': len(submission_df),\n","            'submission_avg': predictions.mean(),\n","            'submission_std': predictions.std(),\n","            'submission_min': predictions.min(),\n","            'submission_max': predictions.max()\n","        })\n","\n","        print(f\"\\n‚úÖ Optimized XGBoost Complete!\")\n","        print(f\"üìä Validation WMAE: {metrics['val_wmae']:.2f}\")\n","        print(f\"üéØ Best Parameters: {metrics['best_params']}\")\n","        print(f\"üìÅ Submission: {submission_filename}\")\n","        print(f\"üîó WandB: {wandb.run.url}\")\n","\n","        return model, submission_df, metrics\n","\n","    except Exception as e:\n","        print(f\"‚ùå Pipeline failed: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        return None\n","\n","    finally:\n","        wandb.finish()"],"metadata":{"id":"NcTnL2BdAsVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TRAIN_PATH = \"data/train.csv\"\n","TEST_PATH = \"data/test.csv\"\n","FEATURES_PATH = \"data/features.csv\"\n","STORES_PATH = \"data/stores.csv\"\n","\n","result = complete_optimized_xgboost_pipeline(TRAIN_PATH, TEST_PATH, FEATURES_PATH, STORES_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":963},"id":"d8k7R1eRBDK3","outputId":"baf546cc-58ae-45f4-d5cc-8d48715deba1","executionInfo":{"status":"error","timestamp":1752004549605,"user_tz":-240,"elapsed":197328,"user":{"displayName":"Badri Losaberidze","userId":"06913732304764740270"}}},"execution_count":8,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["================================================================================\n","WALMART SALES FORECASTING - OPTIMIZED XGBOOST WITH GRIDSEARCH\n","================================================================================\n"]},{"data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n","wandb: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mblosa22\u001b[0m (\u001b[33mblosa22-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.21.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/MLFinal/walmart-sales-forecasting/wandb/run-20250708_172151-qnw5xrh9</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/blosa22-free-university-of-tbilisi-/walmart-forecasting_XGBoost/runs/qnw5xrh9' target=\"_blank\">xgboost-optimized-20250708-172131</a></strong> to <a href='https://wandb.ai/blosa22-free-university-of-tbilisi-/walmart-forecasting_XGBoost' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/blosa22-free-university-of-tbilisi-/walmart-forecasting_XGBoost' target=\"_blank\">https://wandb.ai/blosa22-free-university-of-tbilisi-/walmart-forecasting_XGBoost</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/blosa22-free-university-of-tbilisi-/walmart-forecasting_XGBoost/runs/qnw5xrh9' target=\"_blank\">https://wandb.ai/blosa22-free-university-of-tbilisi-/walmart-forecasting_XGBoost/runs/qnw5xrh9</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["üìÇ Loading data...\n","üìä Preparing optimized data...\n","üîß Creating optimized features...\n","‚úÖ Optimized feature engineering complete. Shape: (421570, 89)\n","üìä Features created: 89 total columns\n","üìä Computing advanced target encodings...\n","üîß Creating optimized features...\n","‚úÖ Optimized feature engineering complete. Shape: (115064, 89)\n","üìä Features created: 89 total columns\n","‚úÖ Optimized data prepared - Train: (421570, 94), Features: 94\n","‚úÖ Test data: (115064, 94)\n","üöÄ Training optimized XGBoost...\n","üîç Starting hyperparameter optimization...\n","üîç Testing 27 parameter combinations...\n","Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">xgboost-optimized-20250708-172131</strong> at: <a href='https://wandb.ai/blosa22-free-university-of-tbilisi-/walmart-forecasting_XGBoost/runs/qnw5xrh9' target=\"_blank\">https://wandb.ai/blosa22-free-university-of-tbilisi-/walmart-forecasting_XGBoost/runs/qnw5xrh9</a><br> View project at: <a href='https://wandb.ai/blosa22-free-university-of-tbilisi-/walmart-forecasting_XGBoost' target=\"_blank\">https://wandb.ai/blosa22-free-university-of-tbilisi-/walmart-forecasting_XGBoost</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250708_172151-qnw5xrh9/logs</code>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-8-1683926244.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mSTORES_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/stores.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplete_optimized_xgboost_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEATURES_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTORES_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-7-659689040.py\u001b[0m in \u001b[0;36mcomplete_optimized_xgboost_pipeline\u001b[0;34m(train_path, test_path, features_path, stores_path)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Train with optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Log metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-6-4053174118.py\u001b[0m in \u001b[0;36mtrain_optimized\u001b[0;34m(self, X_train, y_train, validation_split)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# Optimize hyperparameters first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;31m# Time series split for final evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-6-4053174118.py\u001b[0m in \u001b[0;36moptimize_hyperparameters\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Fit GridSearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}