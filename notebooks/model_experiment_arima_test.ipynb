{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAtelx65D9WL"
   },
   "source": [
    "#Enviroment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25355,
     "status": "ok",
     "timestamp": 1751910815648,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "r268yOnrDcP-",
    "outputId": "427950a4-0010-4503-da41-13b18a3ae6b3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17220,
     "status": "ok",
     "timestamp": 1751818870325,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "Co5xJdmFDsia",
    "outputId": "45ad723c-a5ea-4d25-dfbc-1c9cd582527e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open('/content/drive/MyDrive/MLFinal/git_token.env', 'r') as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "username = \"badrilosaberidze\"\n",
    "\n",
    "%cd /content/drive/MyDrive/MLFinal/walmart-sales-forecasting\n",
    "!git remote set-url origin https://{username}:{token}@github.com/{username}/Walmart-Recruiting---Store-Sales-Forecasting.git\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12667,
     "status": "ok",
     "timestamp": 1751735714623,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "UJW8n3qDF-8s",
    "outputId": "7ff27d83-0748-47d0-9e3c-6fa6ba02d4f7"
   },
   "outputs": [],
   "source": [
    "!pip install dagshub mlflow statsmodels plotly kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9936,
     "status": "ok",
     "timestamp": 1751735728399,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "wtev5D2nF2li",
    "outputId": "cbbe901f-340a-4e3f-f2ea-f79558607ace"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical modeling\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# ML and evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MLflow and DagsHub\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import dagshub\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um_JOhuNI1YI"
   },
   "source": [
    "#Dagshub and MlFlow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267,
     "referenced_widgets": [
      "2d0c976b9cf04fcfb4bc35f294f16c21",
      "af9e939aba884b6198ae792a63047d86"
     ]
    },
    "executionInfo": {
     "elapsed": 5461,
     "status": "ok",
     "timestamp": 1751735739641,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "cDCofvQxIvQz",
    "outputId": "f3f00419-ee63-4e03-c222-fb4f40f4e662"
   },
   "outputs": [],
   "source": [
    "dagshub.init(repo_owner=\"losaberidzebadri\", repo_name=\"Walmart-Recruiting---Store-Sales-Forecasting\", mlflow=True)\n",
    "\n",
    "# Set MLflow tracking URI for DagsHub\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/losaberidzebadri/Walmart-Recruiting---Store-Sales-Forecasting.mlflow\")\n",
    "\n",
    "# Create or set experiment\n",
    "experiment_name = \"ARIMA_Training\"\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "    print(f\"‚úì Created new experiment: {experiment_name}\")\n",
    "except mlflow.exceptions.MlflowException:\n",
    "    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "    print(f\"‚úì Using existing experiment: {experiment_name}\")\n",
    "\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lod4abdzJOth"
   },
   "source": [
    "#Data loading And Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3009,
     "status": "ok",
     "timestamp": 1751818904581,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "irJeRBBPiTdi",
    "outputId": "97f56537-2b58-4835-a784-82e5ea039269"
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1751735742145,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "hTdx0NVIMMZy"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2522,
     "status": "ok",
     "timestamp": 1751735746154,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "66m5iJw-JG3M",
    "outputId": "f976ba21-09f0-4c13-9a7f-3c816fb8bf93"
   },
   "outputs": [],
   "source": [
    "def load_walmart_data():\n",
    "    \"\"\"\n",
    "    Load Walmart sales data and perform initial preprocessing\n",
    "    \"\"\"\n",
    "    print(\"üìä Loading Walmart Sales Data...\")\n",
    "\n",
    "    # Load datasets\n",
    "    train = pd.read_csv('data/train.csv')\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    stores = pd.read_csv('data/stores.csv')\n",
    "    features = pd.read_csv('data/features.csv')\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"Training data shape: {train.shape}\")\n",
    "    print(f\"Test data shape: {test.shape}\")\n",
    "    print(f\"Stores data shape: {stores.shape}\")\n",
    "    print(f\"Features data shape: {features.shape}\")\n",
    "\n",
    "    # Convert dates\n",
    "    train['Date'] = pd.to_datetime(train['Date'])\n",
    "    test['Date'] = pd.to_datetime(test['Date'])\n",
    "    features['Date'] = pd.to_datetime(features['Date'])\n",
    "\n",
    "    # Merge with store information\n",
    "    train = train.merge(stores, on='Store', how='left')\n",
    "    test = test.merge(stores, on='Store', how='left')\n",
    "\n",
    "    # Merge with features\n",
    "    train = train.merge(features, on=['Store', 'Date'], how='left')\n",
    "    test = test.merge(features, on=['Store', 'Date'], how='left')\n",
    "\n",
    "    print(\"‚úì Data loaded and merged successfully\")\n",
    "\n",
    "    return train, test\n",
    "\n",
    "# Load data\n",
    "train_data, test_data = load_walmart_data()\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\nüìà Training Data Overview:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nüìä Data Info:\")\n",
    "print(train_data.info())\n",
    "print(\"\\nüìã Statistical Summary:\")\n",
    "print(train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9109,
     "status": "ok",
     "timestamp": 1751735758877,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "bf19jUPeKGsz",
    "outputId": "d17713d9-2f40-46ec-c091-6526182cb9b4"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"ARIMA_EDA\"):\n",
    "\n",
    "    print(\"\\nüîç EXPLORATORY DATA ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Log basic dataset statistics\n",
    "    mlflow.log_param(\"total_stores\", train_data['Store'].nunique())\n",
    "    mlflow.log_param(\"total_departments\", train_data['Dept'].nunique())\n",
    "    mlflow.log_param(\"date_range_start\", train_data['Date'].min().strftime('%Y-%m-%d'))\n",
    "    mlflow.log_param(\"date_range_end\", train_data['Date'].max().strftime('%Y-%m-%d'))\n",
    "    mlflow.log_param(\"total_observations\", len(train_data))\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_values = train_data.isnull().sum()\n",
    "    print(\"\\nüîç Missing Values:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n",
    "    # Store and Department analysis\n",
    "    print(\"\\nüè™ Store Analysis:\")\n",
    "    store_stats = train_data.groupby('Store').agg({\n",
    "        'Weekly_Sales': ['count', 'mean', 'std', 'min', 'max'],\n",
    "        'Dept': 'nunique'\n",
    "    }).round(2)\n",
    "    print(store_stats.head())\n",
    "\n",
    "    print(\"\\nüõçÔ∏è Department Analysis:\")\n",
    "    dept_stats = train_data.groupby('Dept').agg({\n",
    "        'Weekly_Sales': ['count', 'mean', 'std'],\n",
    "        'Store': 'nunique'\n",
    "    }).round(2)\n",
    "    print(dept_stats.head())\n",
    "\n",
    "    # Visualization 1: Sales distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Sales histogram\n",
    "    axes[0, 0].hist(train_data['Weekly_Sales'], bins=50, alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].set_title('Distribution of Weekly Sales')\n",
    "    axes[0, 0].set_xlabel('Weekly Sales')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "    # Sales by store type\n",
    "    train_data.boxplot(column='Weekly_Sales', by='Type', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Sales by Store Type')\n",
    "    axes[0, 1].set_xlabel('Store Type')\n",
    "    axes[0, 1].set_ylabel('Weekly Sales')\n",
    "\n",
    "    # Sales over time (aggregated)\n",
    "    monthly_sales = train_data.groupby(train_data['Date'].dt.to_period('M'))['Weekly_Sales'].sum()\n",
    "    axes[1, 0].plot(monthly_sales.index.to_timestamp(), monthly_sales.values)\n",
    "    axes[1, 0].set_title('Total Monthly Sales Trend')\n",
    "    axes[1, 0].set_xlabel('Date')\n",
    "    axes[1, 0].set_ylabel('Monthly Sales')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Top departments by sales\n",
    "    top_depts = train_data.groupby('Dept')['Weekly_Sales'].sum().sort_values(ascending=False).head(10)\n",
    "    axes[1, 1].bar(range(len(top_depts)), top_depts.values)\n",
    "    axes[1, 1].set_title('Top 10 Departments by Total Sales')\n",
    "    axes[1, 1].set_xlabel('Department Rank')\n",
    "    axes[1, 1].set_ylabel('Total Sales')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_overview.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Log the plot\n",
    "    mlflow.log_artifact('eda_overview.png')\n",
    "\n",
    "    # Holiday impact analysis\n",
    "    print(\"\\nüéâ Holiday Impact Analysis:\")\n",
    "    holiday_impact = train_data.groupby('IsHoliday_x')['Weekly_Sales'].agg(['mean', 'std', 'count'])\n",
    "    print(holiday_impact)\n",
    "\n",
    "    mlflow.log_metric(\"avg_sales_non_holiday\", holiday_impact.loc[False, 'mean'])\n",
    "    mlflow.log_metric(\"avg_sales_holiday\", holiday_impact.loc[True, 'mean'])\n",
    "\n",
    "    print(\"‚úì EDA completed and logged to MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDrXZqW3Mgg5"
   },
   "source": [
    "#Time Series Selection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1751735765232,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "pc1KcBucMlfK",
    "outputId": "65ce0587-89c8-41f0-f8ae-f4d6ec330c02"
   },
   "outputs": [],
   "source": [
    "def select_time_series_for_arima(data, min_observations=100):\n",
    "    \"\"\"\n",
    "    Select the best time series for ARIMA modeling\n",
    "    Criteria: Most complete data, consistent patterns\n",
    "    \"\"\"\n",
    "    print(\"\\nüéØ SELECTING TIME SERIES FOR ARIMA MODELING\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Analyze completeness for each store-department combination\n",
    "    series_stats = data.groupby(['Store', 'Dept']).agg({\n",
    "        'Weekly_Sales': ['count', 'mean', 'std', 'min', 'max'],\n",
    "        'Date': ['min', 'max']\n",
    "    }).round(2)\n",
    "\n",
    "    # Flatten column names\n",
    "    series_stats.columns = ['_'.join(col).strip() for col in series_stats.columns]\n",
    "\n",
    "    # Filter series with sufficient observations\n",
    "    complete_series = series_stats[series_stats['Weekly_Sales_count'] >= min_observations]\n",
    "\n",
    "    # Rank by completeness and sales volume\n",
    "    complete_series['completeness_score'] = (\n",
    "        complete_series['Weekly_Sales_count'] * 0.4 +\n",
    "        complete_series['Weekly_Sales_mean'] * 0.3 +\n",
    "        (1 / complete_series['Weekly_Sales_std']) * 0.3\n",
    "    )\n",
    "\n",
    "    # Select top series\n",
    "    top_series = complete_series.sort_values('completeness_score', ascending=False).head(10)\n",
    "\n",
    "    print(f\"Found {len(complete_series)} series with >= {min_observations} observations\")\n",
    "    print(\"\\nTop 10 series by completeness score:\")\n",
    "    print(top_series[['Weekly_Sales_count', 'Weekly_Sales_mean', 'completeness_score']])\n",
    "\n",
    "    # Select the best one\n",
    "    best_store, best_dept = top_series.index[0]\n",
    "\n",
    "    print(f\"\\n‚úÖ Selected: Store {best_store}, Department {best_dept}\")\n",
    "\n",
    "    return best_store, best_dept, top_series\n",
    "\n",
    "# Select time series\n",
    "selected_store, selected_dept, series_ranking = select_time_series_for_arima(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eAT2uBRM-oh"
   },
   "source": [
    "##Extract Selected Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1751735774648,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "QlzxStaANBEp",
    "outputId": "0bcc3b5d-306a-4dd0-c977-a209de42ae3b"
   },
   "outputs": [],
   "source": [
    "def extract_time_series(data, store, dept):\n",
    "    \"\"\"Extract and clean time series data\"\"\"\n",
    "    series_data = data[(data['Store'] == store) & (data['Dept'] == dept)].copy()\n",
    "    series_data = series_data.sort_values('Date')\n",
    "    series_data = series_data.set_index('Date')\n",
    "\n",
    "    # Handle missing values\n",
    "    series_data = series_data.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    return series_data\n",
    "\n",
    "# Extract selected series\n",
    "series_df = extract_time_series(train_data, selected_store, selected_dept)\n",
    "sales_series = series_df['Weekly_Sales']\n",
    "\n",
    "print(f\"\\nüìä Selected Time Series Summary:\")\n",
    "print(f\"Store: {selected_store}, Department: {selected_dept}\")\n",
    "print(f\"Observations: {len(sales_series)}\")\n",
    "print(f\"Date range: {sales_series.index.min()} to {sales_series.index.max()}\")\n",
    "print(f\"Mean sales: ${sales_series.mean():,.2f}\")\n",
    "print(f\"Std deviation: ${sales_series.std():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 23475,
     "status": "ok",
     "timestamp": 1751735799467,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "ciySde2ANO3t",
    "outputId": "b670a6ef-2492-4d7a-dcd5-bb002260f1b5"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"ARIMA_Time_Series_Analysis\"):\n",
    "\n",
    "    print(\"\\nüìà TIME SERIES ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Log series parameters\n",
    "    mlflow.log_param(\"selected_store\", selected_store)\n",
    "    mlflow.log_param(\"selected_department\", selected_dept)\n",
    "    mlflow.log_param(\"series_length\", len(sales_series))\n",
    "    mlflow.log_param(\"series_frequency\", \"Weekly\")\n",
    "\n",
    "    # Plot time series\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "    # Original series\n",
    "    axes[0, 0].plot(sales_series.index, sales_series.values, linewidth=1.5, color='blue')\n",
    "    axes[0, 0].set_title(f'Weekly Sales - Store {selected_store}, Dept {selected_dept}')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('Weekly Sales ($)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Rolling statistics\n",
    "    window = 12  # 12 weeks ~ 3 months\n",
    "    rolling_mean = sales_series.rolling(window=window).mean()\n",
    "    rolling_std = sales_series.rolling(window=window).std()\n",
    "\n",
    "    axes[0, 1].plot(sales_series.index, sales_series.values, label='Original', alpha=0.7)\n",
    "    axes[0, 1].plot(rolling_mean.index, rolling_mean.values, label=f'{window}-week MA', color='red')\n",
    "    axes[0, 1].fill_between(rolling_mean.index,\n",
    "                           rolling_mean - rolling_std,\n",
    "                           rolling_mean + rolling_std,\n",
    "                           alpha=0.2, color='red')\n",
    "    axes[0, 1].set_title('Rolling Mean and Standard Deviation')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Seasonal patterns\n",
    "    sales_series_monthly = sales_series.groupby(sales_series.index.month).mean()\n",
    "    axes[1, 0].bar(sales_series_monthly.index, sales_series_monthly.values, color='green', alpha=0.7)\n",
    "    axes[1, 0].set_title('Average Sales by Month')\n",
    "    axes[1, 0].set_xlabel('Month')\n",
    "    axes[1, 0].set_ylabel('Average Weekly Sales ($)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Distribution\n",
    "    axes[1, 1].hist(sales_series.values, bins=30, alpha=0.7, color='purple')\n",
    "    axes[1, 1].set_title('Sales Distribution')\n",
    "    axes[1, 1].set_xlabel('Weekly Sales ($)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('time_series_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"series_mean\", sales_series.mean())\n",
    "    mlflow.log_metric(\"series_std\", sales_series.std())\n",
    "    mlflow.log_metric(\"series_min\", sales_series.min())\n",
    "    mlflow.log_metric(\"series_max\", sales_series.max())\n",
    "    mlflow.log_metric(\"series_skewness\", sales_series.skew())\n",
    "    mlflow.log_metric(\"series_kurtosis\", sales_series.kurtosis())\n",
    "\n",
    "    # Log artifact\n",
    "    mlflow.log_artifact('time_series_analysis.png')\n",
    "\n",
    "    # Seasonal decomposition\n",
    "    print(\"\\nüîç Seasonal Decomposition:\")\n",
    "\n",
    "    # Use additive decomposition first\n",
    "    decomposition = seasonal_decompose(sales_series, model='additive', period=52)\n",
    "\n",
    "    # Plot decomposition\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "\n",
    "    decomposition.observed.plot(ax=axes[0], title='Original Time Series')\n",
    "    decomposition.trend.plot(ax=axes[1], title='Trend Component')\n",
    "    decomposition.seasonal.plot(ax=axes[2], title='Seasonal Component')\n",
    "    decomposition.resid.plot(ax=axes[3], title='Residual Component')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('seasonal_decomposition.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Log decomposition metrics\n",
    "    trend_strength = 1 - np.var(decomposition.resid.dropna()) / np.var(decomposition.observed.dropna())\n",
    "    seasonal_strength = 1 - np.var(decomposition.resid.dropna()) / np.var(decomposition.seasonal.dropna())\n",
    "\n",
    "    mlflow.log_metric(\"trend_strength\", trend_strength)\n",
    "    mlflow.log_metric(\"seasonal_strength\", seasonal_strength)\n",
    "    mlflow.log_artifact('seasonal_decomposition.png')\n",
    "\n",
    "    print(f\"Trend strength: {trend_strength:.3f}\")\n",
    "    print(f\"Seasonal strength: {seasonal_strength:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN1grh5bNlyc"
   },
   "source": [
    "#Stationary Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 46340,
     "status": "ok",
     "timestamp": 1751735847463,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "D-UIPWxSNgQl",
    "outputId": "7b0ab4d1-9819-4160-899d-dc687e451f46"
   },
   "outputs": [],
   "source": [
    "def comprehensive_stationarity_test(series, title=\"Time Series\"):\n",
    "    \"\"\"\n",
    "    Perform comprehensive stationarity testing\n",
    "    \"\"\"\n",
    "    print(f\"\\nüî¨ STATIONARITY TESTING: {title}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Augmented Dickey-Fuller Test\n",
    "    print(\"1. Augmented Dickey-Fuller Test:\")\n",
    "    adf_result = adfuller(series.dropna(), autolag='AIC')\n",
    "\n",
    "    print(f\"   ADF Statistic: {adf_result[0]:.6f}\")\n",
    "    print(f\"   p-value: {adf_result[1]:.6f}\")\n",
    "    print(f\"   Critical Values:\")\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f\"      {key}: {value:.3f}\")\n",
    "\n",
    "    adf_stationary = adf_result[1] <= 0.05\n",
    "    print(f\"   Result: {'‚úì Stationary' if adf_stationary else '‚úó Non-stationary'}\")\n",
    "\n",
    "    # KPSS Test\n",
    "    print(\"\\n2. KPSS Test:\")\n",
    "    kpss_result = kpss(series.dropna(), regression='c')\n",
    "\n",
    "    print(f\"   KPSS Statistic: {kpss_result[0]:.6f}\")\n",
    "    print(f\"   p-value: {kpss_result[1]:.6f}\")\n",
    "    print(f\"   Critical Values:\")\n",
    "    for key, value in kpss_result[3].items():\n",
    "        print(f\"      {key}: {value:.3f}\")\n",
    "\n",
    "    kpss_stationary = kpss_result[1] > 0.05\n",
    "    print(f\"   Result: {'‚úì Stationary' if kpss_stationary else '‚úó Non-stationary'}\")\n",
    "\n",
    "    # Overall conclusion\n",
    "    overall_stationary = adf_stationary and kpss_stationary\n",
    "    print(f\"\\nüìä Overall Stationarity: {'‚úì STATIONARY' if overall_stationary else '‚úó NON-STATIONARY'}\")\n",
    "\n",
    "    results = {\n",
    "        'adf_statistic': adf_result[0],\n",
    "        'adf_pvalue': adf_result[1],\n",
    "        'adf_stationary': adf_stationary,\n",
    "        'kpss_statistic': kpss_result[0],\n",
    "        'kpss_pvalue': kpss_result[1],\n",
    "        'kpss_stationary': kpss_stationary,\n",
    "        'overall_stationary': overall_stationary\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Start MLflow run for stationarity testing\n",
    "with mlflow.start_run(run_name=\"ARIMA_Stationarity_Testing\"):\n",
    "\n",
    "    # Test original series\n",
    "    original_stationarity = comprehensive_stationarity_test(sales_series, \"Original Series\")\n",
    "\n",
    "    # Log results\n",
    "    for key, value in original_stationarity.items():\n",
    "        if isinstance(value, bool):\n",
    "            mlflow.log_param(f\"original_{key}\", value)\n",
    "        else:\n",
    "            mlflow.log_metric(f\"original_{key}\", value)\n",
    "\n",
    "    # Apply differencing if needed\n",
    "    differenced_series = sales_series.copy()\n",
    "    d_order = 0\n",
    "\n",
    "    if not original_stationarity['overall_stationary']:\n",
    "        print(\"\\nüîÑ Applying differencing...\")\n",
    "\n",
    "        # First difference\n",
    "        differenced_series = sales_series.diff().dropna()\n",
    "        d_order = 1\n",
    "\n",
    "        first_diff_stationarity = comprehensive_stationarity_test(differenced_series, \"First Differenced Series\")\n",
    "\n",
    "        # Log first difference results\n",
    "        for key, value in first_diff_stationarity.items():\n",
    "            if isinstance(value, bool):\n",
    "                mlflow.log_param(f\"first_diff_{key}\", value)\n",
    "            else:\n",
    "                mlflow.log_metric(f\"first_diff_{key}\", value)\n",
    "\n",
    "        # Second difference if needed\n",
    "        if not first_diff_stationarity['overall_stationary']:\n",
    "            differenced_series = differenced_series.diff().dropna()\n",
    "            d_order = 2\n",
    "\n",
    "            second_diff_stationarity = comprehensive_stationarity_test(differenced_series, \"Second Differenced Series\")\n",
    "\n",
    "            # Log second difference results\n",
    "            for key, value in second_diff_stationarity.items():\n",
    "                if isinstance(value, bool):\n",
    "                    mlflow.log_param(f\"second_diff_{key}\", value)\n",
    "                else:\n",
    "                    mlflow.log_metric(f\"second_diff_{key}\", value)\n",
    "\n",
    "    mlflow.log_param(\"optimal_d_order\", d_order)\n",
    "\n",
    "    # Plot original vs differenced\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "    axes[0].plot(sales_series.index, sales_series.values, linewidth=1.5, color='blue')\n",
    "    axes[0].set_title('Original Time Series')\n",
    "    axes[0].set_ylabel('Weekly Sales ($)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].plot(differenced_series.index, differenced_series.values, linewidth=1.5, color='red')\n",
    "    axes[1].set_title(f'Differenced Series (d={d_order})')\n",
    "    axes[1].set_ylabel('Differenced Values')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('stationarity_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    mlflow.log_artifact('stationarity_comparison.png')\n",
    "\n",
    "    print(f\"\\n‚úÖ Optimal differencing order: d = {d_order}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjiG7FCZN-R4"
   },
   "source": [
    "#ACF and PACF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4168,
     "status": "ok",
     "timestamp": 1751735858835,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "tX9slfndN-FF",
    "outputId": "832460b4-6168-44ba-8eed-dad3d0924758"
   },
   "outputs": [],
   "source": [
    "def plot_acf_pacf_analysis(series, lags=40, title=\"ACF/PACF Analysis\"):\n",
    "    \"\"\"\n",
    "    Plot ACF and PACF for parameter identification\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä {title}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # ACF\n",
    "    plot_acf(series.dropna(), lags=lags, ax=axes[0, 0], alpha=0.05)\n",
    "    axes[0, 0].set_title('Autocorrelation Function (ACF)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # PACF\n",
    "    plot_pacf(series.dropna(), lags=lags, ax=axes[0, 1], alpha=0.05)\n",
    "    axes[0, 1].set_title('Partial Autocorrelation Function (PACF)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # ACF of squared series (for ARCH effects)\n",
    "    plot_acf(series.dropna()**2, lags=lags, ax=axes[1, 0], alpha=0.05)\n",
    "    axes[1, 0].set_title('ACF of Squared Series')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Q-Q plot\n",
    "    from scipy import stats\n",
    "    stats.probplot(series.dropna(), dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title('Q-Q Plot (Normality Check)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('acf_pacf_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Start MLflow run for ACF/PACF analysis\n",
    "with mlflow.start_run(run_name=\"ARIMA_ACF_PACF_Analysis\"):\n",
    "\n",
    "    # Analyze the stationary series\n",
    "    stationary_series = differenced_series if d_order > 0 else sales_series\n",
    "\n",
    "    # Plot ACF/PACF\n",
    "    acf_pacf_fig = plot_acf_pacf_analysis(stationary_series, lags=52, title=\"Parameter Identification\")\n",
    "\n",
    "    # Log the plot\n",
    "    mlflow.log_artifact('acf_pacf_analysis.png')\n",
    "\n",
    "    # Ljung-Box test for autocorrelation\n",
    "    print(\"\\nüîç Ljung-Box Test for Autocorrelation:\")\n",
    "    ljung_box_result = acorr_ljungbox(stationary_series.dropna(), lags=10, return_df=True)\n",
    "    print(ljung_box_result)\n",
    "\n",
    "    # Log Ljung-Box results\n",
    "    mlflow.log_metric(\"ljung_box_statistic\", ljung_box_result['lb_stat'].iloc[-1])\n",
    "    mlflow.log_metric(\"ljung_box_pvalue\", ljung_box_result['lb_pvalue'].iloc[-1])\n",
    "\n",
    "    print(\"\\nüí° Parameter Identification Guidelines:\")\n",
    "    print(\"‚Ä¢ AR(p): PACF cuts off after lag p, ACF decays exponentially\")\n",
    "    print(\"‚Ä¢ MA(q): ACF cuts off after lag q, PACF decays exponentially\")\n",
    "    print(\"‚Ä¢ ARMA(p,q): Both ACF and PACF decay exponentially\")\n",
    "    print(\"‚Ä¢ Seasonal patterns: Look for spikes at seasonal lags (52 for weekly data)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNQQeMdfOa39"
   },
   "source": [
    "#Model Selection And Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1751735861035,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "xQSVWSTKOWo4"
   },
   "outputs": [],
   "source": [
    "def grid_search_arima(series, max_p=3, max_d=2, max_q=3, seasonal=True, max_P=2, max_D=1, max_Q=2, s=52,\n",
    "                     test_size=0.2, experiment_name=\"ARIMA_Grid_Search\"):\n",
    "    \"\"\"\n",
    "    Grid search for optimal ARIMA parameters with MLflow logging\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Time series data\n",
    "    max_p, max_d, max_q : int\n",
    "        Maximum values for non-seasonal parameters\n",
    "    seasonal : bool\n",
    "        Whether to include seasonal components\n",
    "    max_P, max_D, max_Q : int\n",
    "        Maximum values for seasonal parameters\n",
    "    s : int\n",
    "        Seasonal period\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    experiment_name : str\n",
    "        MLflow experiment name\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Best model results and fitted model\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç ARIMA GRID SEARCH WITH MLFLOW LOGGING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Set up MLflow experiment\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Split data for validation\n",
    "    split_point = int(len(series) * (1 - test_size))\n",
    "    train_data = series[:split_point]\n",
    "    test_data = series[split_point:]\n",
    "\n",
    "    print(f\"Training data: {len(train_data)} observations\")\n",
    "    print(f\"Test data: {len(test_data)} observations\")\n",
    "\n",
    "    best_aic = float('inf')\n",
    "    best_params = None\n",
    "    best_seasonal_params = None\n",
    "    best_model = None\n",
    "    results = []\n",
    "\n",
    "    total_combinations = (max_p + 1) * (max_d + 1) * (max_q + 1)\n",
    "    if seasonal:\n",
    "        total_combinations *= (max_P + 1) * (max_D + 1) * (max_Q + 1)\n",
    "\n",
    "    print(f\"Testing {total_combinations} parameter combinations...\")\n",
    "\n",
    "    combination_count = 0\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"ARIMA_GridSearch_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
    "        # Log experiment parameters\n",
    "        mlflow.log_param(\"max_p\", max_p)\n",
    "        mlflow.log_param(\"max_d\", max_d)\n",
    "        mlflow.log_param(\"max_q\", max_q)\n",
    "        mlflow.log_param(\"seasonal\", seasonal)\n",
    "        if seasonal:\n",
    "            mlflow.log_param(\"max_P\", max_P)\n",
    "            mlflow.log_param(\"max_D\", max_D)\n",
    "            mlflow.log_param(\"max_Q\", max_Q)\n",
    "            mlflow.log_param(\"seasonal_period\", s)\n",
    "        mlflow.log_param(\"test_size\", test_size)\n",
    "        mlflow.log_param(\"total_combinations\", total_combinations)\n",
    "\n",
    "        for p in range(max_p + 1):\n",
    "            for d in range(max_d + 1):\n",
    "                for q in range(max_q + 1):\n",
    "\n",
    "                    if seasonal:\n",
    "                        for P in range(max_P + 1):\n",
    "                            for D in range(max_D + 1):\n",
    "                                for Q in range(max_Q + 1):\n",
    "                                    combination_count += 1\n",
    "\n",
    "                                    # Create nested run for each model\n",
    "                                    with mlflow.start_run(nested=True,\n",
    "                                                        run_name=f\"SARIMA_({p},{d},{q})_({P},{D},{Q},{s})\"):\n",
    "                                        try:\n",
    "                                            # Fit SARIMA model\n",
    "                                            model = SARIMAX(train_data,\n",
    "                                                           order=(p, d, q),\n",
    "                                                           seasonal_order=(P, D, Q, s),\n",
    "                                                           enforce_stationarity=False,\n",
    "                                                           enforce_invertibility=False)\n",
    "\n",
    "                                            fitted_model = model.fit(disp=False, maxiter=100)\n",
    "\n",
    "                                            # Get metrics\n",
    "                                            aic = fitted_model.aic\n",
    "                                            bic = fitted_model.bic\n",
    "                                            llf = fitted_model.llf\n",
    "\n",
    "                                            # Make predictions for validation\n",
    "                                            forecast = fitted_model.forecast(steps=len(test_data))\n",
    "\n",
    "                                            # Calculate validation metrics\n",
    "                                            mae = mean_absolute_error(test_data, forecast)\n",
    "                                            mse = mean_squared_error(test_data, forecast)\n",
    "                                            rmse = np.sqrt(mse)\n",
    "                                            mape = np.mean(np.abs((test_data - forecast) / test_data)) * 100\n",
    "\n",
    "                                            # Log parameters\n",
    "                                            mlflow.log_param(\"p\", p)\n",
    "                                            mlflow.log_param(\"d\", d)\n",
    "                                            mlflow.log_param(\"q\", q)\n",
    "                                            mlflow.log_param(\"P\", P)\n",
    "                                            mlflow.log_param(\"D\", D)\n",
    "                                            mlflow.log_param(\"Q\", Q)\n",
    "                                            mlflow.log_param(\"s\", s)\n",
    "                                            mlflow.log_param(\"model_type\", \"SARIMA\")\n",
    "\n",
    "                                            # Log metrics\n",
    "                                            mlflow.log_metric(\"AIC\", aic)\n",
    "                                            mlflow.log_metric(\"BIC\", bic)\n",
    "                                            mlflow.log_metric(\"LogLikelihood\", llf)\n",
    "                                            mlflow.log_metric(\"MAE\", mae)\n",
    "                                            mlflow.log_metric(\"MSE\", mse)\n",
    "                                            mlflow.log_metric(\"RMSE\", rmse)\n",
    "                                            mlflow.log_metric(\"MAPE\", mape)\n",
    "                                            mlflow.log_metric(\"converged\", int(fitted_model.mle_retvals['converged']))\n",
    "\n",
    "                                            # Log model\n",
    "                                            mlflow.statsmodels.log_model(fitted_model, \"model\")\n",
    "\n",
    "                                            results.append({\n",
    "                                                'p': p, 'd': d, 'q': q,\n",
    "                                                'P': P, 'D': D, 'Q': Q, 's': s,\n",
    "                                                'AIC': aic, 'BIC': bic, 'LogLikelihood': llf,\n",
    "                                                'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'MAPE': mape,\n",
    "                                                'converged': fitted_model.mle_retvals['converged'],\n",
    "                                                'model_type': 'SARIMA'\n",
    "                                            })\n",
    "\n",
    "                                            if aic < best_aic:\n",
    "                                                best_aic = aic\n",
    "                                                best_params = (p, d, q)\n",
    "                                                best_seasonal_params = (P, D, Q, s)\n",
    "                                                best_model = fitted_model\n",
    "\n",
    "                                            if combination_count % 50 == 0:\n",
    "                                                print(f\"Tested {combination_count}/{total_combinations} combinations...\")\n",
    "\n",
    "                                        except Exception as e:\n",
    "                                            mlflow.log_param(\"error\", str(e))\n",
    "                                            continue\n",
    "                    else:\n",
    "                        combination_count += 1\n",
    "\n",
    "                        # Create nested run for each model\n",
    "                        with mlflow.start_run(nested=True, run_name=f\"ARIMA_({p},{d},{q})\"):\n",
    "                            try:\n",
    "                                # Fit ARIMA model\n",
    "                                model = ARIMA(train_data, order=(p, d, q))\n",
    "                                fitted_model = model.fit()\n",
    "\n",
    "                                # Get metrics\n",
    "                                aic = fitted_model.aic\n",
    "                                bic = fitted_model.bic\n",
    "                                llf = fitted_model.llf\n",
    "\n",
    "                                # Make predictions for validation\n",
    "                                forecast = fitted_model.forecast(steps=len(test_data))\n",
    "\n",
    "                                # Calculate validation metrics\n",
    "                                mae = mean_absolute_error(test_data, forecast)\n",
    "                                mse = mean_squared_error(test_data, forecast)\n",
    "                                rmse = np.sqrt(mse)\n",
    "                                mape = np.mean(np.abs((test_data - forecast) / test_data)) * 100\n",
    "\n",
    "                                # Log parameters\n",
    "                                mlflow.log_param(\"p\", p)\n",
    "                                mlflow.log_param(\"d\", d)\n",
    "                                mlflow.log_param(\"q\", q)\n",
    "                                mlflow.log_param(\"model_type\", \"ARIMA\")\n",
    "\n",
    "                                # Log metrics\n",
    "                                mlflow.log_metric(\"AIC\", aic)\n",
    "                                mlflow.log_metric(\"BIC\", bic)\n",
    "                                mlflow.log_metric(\"LogLikelihood\", llf)\n",
    "                                mlflow.log_metric(\"MAE\", mae)\n",
    "                                mlflow.log_metric(\"MSE\", mse)\n",
    "                                mlflow.log_metric(\"RMSE\", rmse)\n",
    "                                mlflow.log_metric(\"MAPE\", mape)\n",
    "\n",
    "                                # Log model\n",
    "                                mlflow.statsmodels.log_model(fitted_model, \"model\")\n",
    "\n",
    "                                results.append({\n",
    "                                    'p': p, 'd': d, 'q': q,\n",
    "                                    'P': None, 'D': None, 'Q': None, 's': None,\n",
    "                                    'AIC': aic, 'BIC': bic, 'LogLikelihood': llf,\n",
    "                                    'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'MAPE': mape,\n",
    "                                    'converged': True,\n",
    "                                    'model_type': 'ARIMA'\n",
    "                                })\n",
    "\n",
    "                                if aic < best_aic:\n",
    "                                    best_aic = aic\n",
    "                                    best_params = (p, d, q)\n",
    "                                    best_seasonal_params = None\n",
    "                                    best_model = fitted_model\n",
    "\n",
    "                                if combination_count % 20 == 0:\n",
    "                                    print(f\"Tested {combination_count}/{total_combinations} combinations...\")\n",
    "\n",
    "                            except Exception as e:\n",
    "                                mlflow.log_param(\"error\", str(e))\n",
    "                                continue\n",
    "\n",
    "        # Log best model results\n",
    "        mlflow.log_metric(\"best_AIC\", best_aic)\n",
    "        if best_params:\n",
    "            mlflow.log_param(\"best_p\", best_params[0])\n",
    "            mlflow.log_param(\"best_d\", best_params[1])\n",
    "            mlflow.log_param(\"best_q\", best_params[2])\n",
    "        if best_seasonal_params:\n",
    "            mlflow.log_param(\"best_P\", best_seasonal_params[0])\n",
    "            mlflow.log_param(\"best_D\", best_seasonal_params[1])\n",
    "            mlflow.log_param(\"best_Q\", best_seasonal_params[2])\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    print(f\"\\n‚úÖ Grid search completed!\")\n",
    "    print(f\"Best parameters: ARIMA{best_params}\")\n",
    "    if best_seasonal_params:\n",
    "        print(f\"Best seasonal parameters: {best_seasonal_params}\")\n",
    "    print(f\"Best AIC: {best_aic:.4f}\")\n",
    "\n",
    "    # Display top 10 models\n",
    "    print(f\"\\nüèÜ TOP 10 MODELS BY AIC:\")\n",
    "    print(\"-\" * 80)\n",
    "    top_models = results_df.nsmallest(10, 'AIC')\n",
    "    for idx, row in top_models.iterrows():\n",
    "        if row['model_type'] == 'SARIMA':\n",
    "            print(f\"SARIMA({row['p']},{row['d']},{row['q']})({row['P']},{row['D']},{row['Q']},{row['s']}) - \"\n",
    "                  f\"AIC: {row['AIC']:.4f}, RMSE: {row['RMSE']:.4f}, MAPE: {row['MAPE']:.2f}%\")\n",
    "        else:\n",
    "            print(f\"ARIMA({row['p']},{row['d']},{row['q']}) - \"\n",
    "                  f\"AIC: {row['AIC']:.4f}, RMSE: {row['RMSE']:.4f}, MAPE: {row['MAPE']:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params': best_params,\n",
    "        'best_seasonal_params': best_seasonal_params,\n",
    "        'best_aic': best_aic,\n",
    "        'results_df': results_df,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1751735865821,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "FeUXPC1vQt1Z"
   },
   "outputs": [],
   "source": [
    "def fit_best_arima_model(series, params, seasonal_params=None, mlflow_run_name=\"Best_ARIMA_Model\"):\n",
    "    \"\"\"\n",
    "    Fit the best ARIMA model found from grid search\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Full time series data\n",
    "    params : tuple\n",
    "        ARIMA parameters (p, d, q)\n",
    "    seasonal_params : tuple, optional\n",
    "        Seasonal parameters (P, D, Q, s)\n",
    "    mlflow_run_name : str\n",
    "        Name for MLflow run\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    fitted model and results\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ FITTING BEST ARIMA MODEL\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    with mlflow.start_run(run_name=mlflow_run_name):\n",
    "        try:\n",
    "            if seasonal_params:\n",
    "                # Fit SARIMA\n",
    "                model = SARIMAX(series,\n",
    "                               order=params,\n",
    "                               seasonal_order=seasonal_params,\n",
    "                               enforce_stationarity=False,\n",
    "                               enforce_invertibility=False)\n",
    "                model_type = \"SARIMA\"\n",
    "                print(f\"Fitting SARIMA{params}{seasonal_params}...\")\n",
    "            else:\n",
    "                # Fit ARIMA\n",
    "                model = ARIMA(series, order=params)\n",
    "                model_type = \"ARIMA\"\n",
    "                print(f\"Fitting ARIMA{params}...\")\n",
    "\n",
    "            fitted_model = model.fit(disp=False)\n",
    "\n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"p\", params[0])\n",
    "            mlflow.log_param(\"d\", params[1])\n",
    "            mlflow.log_param(\"q\", params[2])\n",
    "            mlflow.log_param(\"model_type\", model_type)\n",
    "\n",
    "            if seasonal_params:\n",
    "                mlflow.log_param(\"P\", seasonal_params[0])\n",
    "                mlflow.log_param(\"D\", seasonal_params[1])\n",
    "                mlflow.log_param(\"Q\", seasonal_params[2])\n",
    "                mlflow.log_param(\"s\", seasonal_params[3])\n",
    "\n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"AIC\", fitted_model.aic)\n",
    "            mlflow.log_metric(\"BIC\", fitted_model.bic)\n",
    "            mlflow.log_metric(\"LogLikelihood\", fitted_model.llf)\n",
    "            mlflow.log_metric(\"converged\", int(fitted_model.mle_retvals.get('converged', True)))\n",
    "\n",
    "            # Log model\n",
    "            mlflow.statsmodels.log_model(fitted_model, \"best_model\")\n",
    "\n",
    "            # Log model summary\n",
    "            summary_text = str(fitted_model.summary())\n",
    "            mlflow.log_text(summary_text, \"model_summary.txt\")\n",
    "\n",
    "            print(f\"‚úÖ Model fitted successfully!\")\n",
    "            print(f\"AIC: {fitted_model.aic:.4f}\")\n",
    "            print(f\"BIC: {fitted_model.bic:.4f}\")\n",
    "\n",
    "            return fitted_model\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error fitting model: {e}\")\n",
    "            mlflow.log_param(\"error\", str(e))\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1751735872291,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "FCdLNqktQ4LF"
   },
   "outputs": [],
   "source": [
    "def evaluate_arima_model(fitted_model, test_data, forecast_steps=None):\n",
    "    \"\"\"\n",
    "    Evaluate ARIMA model performance\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    fitted_model : fitted ARIMA/SARIMA model\n",
    "    test_data : pd.Series\n",
    "        Test data for evaluation\n",
    "    forecast_steps : int, optional\n",
    "        Number of steps to forecast (default: len(test_data))\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Evaluation metrics and forecasts\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä EVALUATING MODEL PERFORMANCE\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    if forecast_steps is None:\n",
    "        forecast_steps = len(test_data)\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Model_Evaluation\"):\n",
    "        try:\n",
    "            # Generate forecasts\n",
    "            forecast = fitted_model.forecast(steps=forecast_steps)\n",
    "            forecast_ci = fitted_model.get_forecast(steps=forecast_steps).conf_int()\n",
    "\n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(test_data, forecast)\n",
    "            mse = mean_squared_error(test_data, forecast)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mape = np.mean(np.abs((test_data - forecast) / test_data)) * 100\n",
    "\n",
    "            # Log evaluation metrics\n",
    "            mlflow.log_metric(\"eval_MAE\", mae)\n",
    "            mlflow.log_metric(\"eval_MSE\", mse)\n",
    "            mlflow.log_metric(\"eval_RMSE\", rmse)\n",
    "            mlflow.log_metric(\"eval_MAPE\", mape)\n",
    "\n",
    "            print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "            print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")\n",
    "            print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "\n",
    "            return {\n",
    "                'forecast': forecast,\n",
    "                'forecast_ci': forecast_ci,\n",
    "                'mae': mae,\n",
    "                'mse': mse,\n",
    "                'rmse': rmse,\n",
    "                'mape': mape\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error evaluating model: {e}\")\n",
    "            mlflow.log_param(\"evaluation_error\", str(e))\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1751735874476,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "pJ2dvLj8Q64M"
   },
   "outputs": [],
   "source": [
    "def run_arima_pipeline(series, experiment_name=\"ARIMA_Time_Series_Analysis\"):\n",
    "    \"\"\"\n",
    "    Complete ARIMA modeling pipeline with MLflow logging\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Time series data\n",
    "    experiment_name : str\n",
    "        MLflow experiment name\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Complete pipeline results\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ RUNNING COMPLETE ARIMA PIPELINE\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Step 1: Grid search for best parameters\n",
    "    grid_results = grid_search_arima(\n",
    "        series=series,\n",
    "        max_p=3, max_d=2, max_q=3,\n",
    "        seasonal=True, max_P=2, max_D=1, max_Q=2, s=52,\n",
    "        experiment_name=experiment_name\n",
    "    )\n",
    "\n",
    "    # Step 2: Fit best model on full data\n",
    "    best_model = fit_best_arima_model(\n",
    "        series=series,\n",
    "        params=grid_results['best_params'],\n",
    "        seasonal_params=grid_results['best_seasonal_params']\n",
    "    )\n",
    "\n",
    "    # Step 3: Evaluate model\n",
    "    evaluation = evaluate_arima_model(\n",
    "        fitted_model=best_model,\n",
    "        test_data=grid_results['test_data']\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'grid_search_results': grid_results,\n",
    "        'best_fitted_model': best_model,\n",
    "        'evaluation_results': evaluation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 86118,
     "status": "error",
     "timestamp": 1751735962576,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "S9hz8v7dQ75F",
    "outputId": "b3c46676-00d4-439c-fadc-2ef16605aba7"
   },
   "outputs": [],
   "source": [
    "# Replace 'your_stationary_series' with your actual data\n",
    "results = run_arima_pipeline(\n",
    "    series=stationary_series\n",
    ")\n",
    "\n",
    "# Access the best model\n",
    "best_model = results['best_fitted_model']\n",
    "print(f\"Best model: {results['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7088,
     "status": "ok",
     "timestamp": 1751910824096,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "x3k9D7AWBQnE",
    "outputId": "20da296f-07ff-4319-be4c-0ffbd4e314b6"
   },
   "outputs": [],
   "source": [
    "!pip install nbstripout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1751910842583,
     "user": {
      "displayName": "Badri Losaberidze",
      "userId": "06913732304764740270"
     },
     "user_tz": -240
    },
    "id": "L8ZI5cTOBcv3",
    "outputId": "8975a016-7000-4efc-ec9b-9538e82165ea"
   },
   "outputs": [],
   "source": [
    "!nbstripout /content/drive/MyDrive/MLFinal/model_experiment_arima_test.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOjnuG3uj4VJLVm9N1gTe1A",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
